deployment:
  docker_image: "lmsysorg/sglang:v0.4.9.post4-cu126"
  container_name: "sglang"
  port: 8080
  
  # Universal Docker parameters
  docker_params:
    gpus: "all"
    shm-size: "1000g"
    ipc: "host"
    network: "host"
    volume:
      - "/opt/dlami/nvme/:/sgl-workspace/sglang/model"
  command: "python3 -m sglang.launch_server"
  # Universal application arguments
  app_args:
    host: "0.0.0.0"
    model-path: "model/Qwen/Qwen3-235B-A22B-FP8"
    trust-remote-code: true
    dp-size: 2
    tp-size: 4
    mem-fraction-static: 0.90
    tool-call-parser: "qwen25"
    reasoning-parser: "deepseek-r1"

test_matrix:
  input_tokens: [1600, 6400, 12800]
  output_tokens: [100, 400, 1000]
  processing_num: [1, 16, 32, 64, 128]
  random_tokens: [100, 1600, 6400]

test_config:
  requests_per_process: 5
  warmup_requests: 1
  cooldown_seconds: 5