{
  "aws_ec2_prices": {
    "g5.2xlarge": 1.2120,
    "g5.4xlarge": 1.6240,
    "g5.8xlarge": 2.4480,
    "g5.12xlarge": 5.6720,
    "g5.16xlarge": 4.0960,
    "g5.24xlarge": 8.1440,
    "g5.48xlarge": 16.2880,
    "g6e.2xlarge": 2.24208,
    "g6e.4xlarge": 3.00424,
    "g6e.8xlarge": 4.52856,
    "g6e.12xlarge": 10.49264,
    "g6e.16xlarge": 7.57719,
    "g6e.24xlarge": 15.06559,
    "g6e.48xlarge": 30.13118,
    "p4d.24xlarge": 11.79166,
    "p4de.24xlarge": 14.75,
    "p5.4xlarge": 3.91666,
    "p5.48xlarge": 31.45833,
    "p5e.48xlarge": 34.625,
    "p5en.48xlarge": 36.16666,
    "p6-b200.48xlarge": 65.125,
    "H20-96G": 6.80555,
    "H20-141G": 47.58333
  },
  "default_region": "us-west-2",
  "last_updated": "2025-08-12",
  "notes": {
    "g5": "NVIDIA A10G GPU instances - good for inference workloads",
    "g6e": "NVIDIA L40s GPU instances - good for inference workloads",
    "p4d": "NVIDIA A100 GPU instances - high performance training and inference",
    "p4de": "NVIDIA A100 GPU instances - high performance training and inference",
    "p5": "NVIDIA H100 GPU instances - latest generation for AI workloads",
    "p5en": "NVIDIA H200 GPU instances with enhanced networking",
    "p6-b200": "NVIDIA B200 GPU instances - next generation AI accelerators"
  }
}
