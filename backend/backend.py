# æ ‡å‡†åº“
import base64
import hashlib
import io
import json
import logging
import os
import queue
import re
import subprocess
import tempfile
import threading
import time
import traceback
from datetime import datetime

# ç¬¬ä¸‰æ–¹åº“ - Webç›¸å…³
from flask import Flask, request, jsonify, Response, make_response
from flask_cors import CORS
import requests

# ç¬¬ä¸‰æ–¹åº“ - AWSç›¸å…³
import boto3

# ç¬¬ä¸‰æ–¹åº“ - æ•°æ®å¤„ç†
from PIL import Image

# ç¬¬ä¸‰æ–¹åº“ - AI/MLç›¸å…³
from openai import OpenAI
from emd.sdk.clients.sagemaker_client import SageMakerClient
from emd.sdk.status import get_model_status
from emd.sdk.bootstrap import bootstrap
from emd.sdk.deploy import deploy

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='backend.log',
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

bedrock_client = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-west-2'
)

deployment_threads = {}
# EMD client configurations - Updated with tested models
EMD_MODELS = {
    'qwen2-vl-7b': {
        "model_path": 'Qwen2-VL-7B-Instruct',
        "name": "Qwen2-VL-7B",
        "description": "é€šä¹‰åƒé—®è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œ7Bå‚æ•°",
    },
    'qwen2.5-vl-7b': {
        "model_path": 'Qwen2.5-VL-7B-Instruct',
        "name": "Qwen2.5-VL-7B",
        "description": "é€šä¹‰åƒé—®è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œ7Bå‚æ•°",
    },
    'qwen2.5-vl-32b': {
        "model_path": 'Qwen2.5-VL-32B-Instruct',
        "name": "Qwen2.5-VL-32B",
        "description": "é€šä¹‰åƒé—®è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œ32Bå‚æ•°",
    },
    'qwen2.5-0.5b': {
        "model_path": 'Qwen2.5-0.5B-Instruct',
        "name": "Qwen2.5-0.5B",
        "description": "è½»é‡çº§æ–‡æœ¬æ¨¡å‹ï¼Œé€‚åˆå¿«é€Ÿæ¨ç†",
    },
    'gemma-3-4b': {
        "model_path": 'gemma-3-4b-it',
        "name": "Gemma-3-4B",
        "description": "Googleå¼€æºè¯­è¨€æ¨¡å‹",
    },
    'ui-tars-1.5-7b': {
        "model_path": 'UI-TARS-1.5-7B',
        "name": "UI-TARS-1.5-7B",
        "description": "ç”¨æˆ·ç•Œé¢ç†è§£ä¸“ç”¨æ¨¡å‹",
    },
    'qwen3-0.6b': {
        "model_path": 'Qwen3-0.6B',
        "name": "Qwen3-0.6B",
        "description": "æœ€æ–°Qwen3æ¨¡å‹ï¼Œ0.6Bå‚æ•°ï¼Œé«˜æ•ˆè½»é‡",
    },
    'qwen3-8b': {
        "model_path": 'Qwen3-8B',
        "name": "Qwen3-8B",
        "description": "æœ€æ–°Qwen3æ¨¡å‹ï¼Œ8Bå‚æ•°ï¼Œå¼ºå¤§æ€§èƒ½",
    }
}

BEDROCK_MODELS = {
    'claude4': {
        "name": "Claude 4",
        "description": "æœ€æ–°çš„Claudeæ¨¡å‹ï¼Œå…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›",
    }, 
    'claude35': {
        "name": "Claude 3.5 Sonnet",
        "description": "å¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦çš„é«˜æ•ˆæ¨¡å‹",
    },
    'nova': {
        "name": "Amazon Nova Pro",
        "description": "AWSåŸç”Ÿå¤šæ¨¡æ€å¤§æ¨¡å‹",
    }
}

ALL_MODELS = {
    "bedrock": BEDROCK_MODELS,
    "emd": EMD_MODELS,
}

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def init_emd_env(region="us-west-2"):
    """Initialize EMD environment by setting region and bootstrapping."""
    # os.environ["AWS_DEFAULT_REGION"] = region
    # subprocess.run(["emd", "bootstrap"], check=True, env=os.environ)
    # logging.info(f"EMD environment initialized in region {region}")
    bootstrap()


def generate_short_tag(model_name: str):
    """
    ç”Ÿæˆä¸€ä¸ªç®€çŸ­ä½†å”¯ä¸€çš„ tagï¼š
    - model_name ä¿ç•™å‰10ä½çš„å®‰å…¨å­—ç¬¦ï¼ˆåªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼‰
    - åç¼€æ·»åŠ æ—¶é—´å’Œå“ˆå¸Œ
    """
    # ä¿ç•™å­—æ¯å’Œæ•°å­—
    safe_prefix = ''.join(c for c in model_name if c.isalnum())[:10].lower()
    
    # å½“å‰æ—¶é—´æˆ³ï¼ˆå¦‚08071345ï¼‰
    time_part = datetime.now().strftime("%m%d%H%M")

    # å“ˆå¸ŒçŸ­ç ï¼ˆé¿å…é‡åï¼‰ï¼šå– model_name + å½“å‰æ—¶é—´ çš„ SHA1 å‰6ä½
    hash_suffix = hashlib.sha1(f"{model_name}{time_part}".encode()).hexdigest()[:6]

    tag = f"{safe_prefix}-{time_part}-{hash_suffix}"
    return tag


def get_current_models():
    print(f"get_current_models")
    """é€šè¿‡ emd status æå– Base URLï¼Œå¹¶è¯·æ±‚ /models æ¥å£ï¼Œè¿”å› CREATE_COMPLETE çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        # è·å– emd status è¾“å‡ºå¹¶æå– Base URL
        # result = subprocess.run(
        #     ["emd", "status"],
        #     capture_output=True,
        #     text=True,
        #     check=True,
        #     env=os.environ
        # )
        # output = result.stdout
        # match = re.search(r"http://[^\s]+\.elb\.amazonaws\.com/v1", output)
        # if not match:
        #     logging.error("âŒ æ— æ³•åœ¨ emd status è¾“å‡ºä¸­æ‰¾åˆ° Base URL")
        #     return {}
        # base_url = match.group(0)
        # logging.info(f"ğŸŒ æ£€æµ‹åˆ° EMD Base URL: {base_url}")

        # # è¯·æ±‚ /models æ¥å£è·å–æ‰€æœ‰æ¨¡å‹
        # response = requests.get(f"{base_url}/models")
        # response.raise_for_status()
        # models_data = response.json().get("data", [])
        # print(f"models_data:{models_data}")
        # è¿‡æ»¤çŠ¶æ€ä¸º CREATE_COMPLETE çš„æ¨¡å‹ï¼Œå¹¶æ„é€ è¿”å›å€¼
        # åˆ›å»ºåå‘æ˜ å°„ï¼šå®Œæ•´æ¨¡å‹å -> ç®€åŒ–é”®å
        status = get_model_status()
        print("status", status)
        logging.info(f"[DEBUG] EMD SDK get_model_statusè¿”å›: {status}")
        reverse_mapping = {v["model_path"]: k for k, v in EMD_MODELS.items()}
        print("reverse_mapping", reverse_mapping)
        
        deployed = {}
        inprogress = {}
        failed = {}

        for model in status["completed"]:
            model_id = model.get("model_id")
            model_tag = model.get("model_tag")
            stack_status = model.get("stack_status")
            
            if model_id in reverse_mapping:
                simple_key = reverse_mapping[model_id]
                if stack_status is not None and "CREATE_COMPLETE" in stack_status:
                    deployed[simple_key] = {
                        "tag": model_tag,
                        "full_name": model_id
                    }
                else:
                    failed[simple_key] = {
                        "tag": model_tag,
                        "full_name": model_id
                    }
        
        for model in status["inprogress"]:
            model_id = model.get("model_id")
            model_tag = model.get("model_tag")
            stack_status = model.get("stack_status")
            model_status = model.get("status")
            execution_info = model.get("execution_info", {})
            execution_status = execution_info.get("status")
            
            if model_id in reverse_mapping:
                simple_key = reverse_mapping[model_id]
                # Check if the inprogress model has actually failed
                is_failed = False
                
                # Check various failure indicators
                if stack_status is not None and any(fail_status in stack_status for fail_status in ["ROLLBACK_COMPLETE", "CREATE_FAILED", "DELETE_FAILED", "ROLLBACK_FAILED"]):
                    is_failed = True
                elif model_status == "Failed":
                    is_failed = True
                elif execution_status == "Failed":
                    is_failed = True
                
                if is_failed:
                    failed[simple_key] = {
                        "tag": model_tag,
                        "full_name": model_id
                    }
                else:
                    inprogress[simple_key] = {
                        "tag": model_tag,
                        "full_name": model_id
                    }
        
        # Also check if there's a "failed" category in the status
        if "failed" in status:
            for model in status["failed"]:
                model_id = model.get("model_id")
                model_tag = model.get("model_tag")
                
                if model_id in reverse_mapping:
                    simple_key = reverse_mapping[model_id]
                    failed[simple_key] = {
                        "tag": model_tag,
                        "full_name": model_id
                    }
        logging.info(f"âœ… å½“å‰éƒ¨ç½²çš„æ¨¡å‹: {deployed}")
        logging.info(f"âœ… å½“å‰æ­£åœ¨éƒ¨ç½²çš„æ¨¡å‹: {inprogress}")
        logging.info(f"âœ… å½“å‰éƒ¨ç½²å¤±è´¥çš„æ¨¡å‹: {failed}")

        return {
            "inprogress": inprogress,
            "deployed": deployed,
            "failed": failed,
        }

    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ è·å– emd status å¤±è´¥: {e}")
        return {}
    except Exception as e:
        logging.error(f"âŒ å¤„ç†æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return {}



def deploy_model_if_not_exist(model_name, instance_type="g5.12xlarge", engine_type="vllm"):
    """Deploy the model if it does not exist."""
    deployed = get_current_models()
    if model_name in deployed:
        logging.info(f"âœ… {model_name} is already deployed with tag: {deployed[model_name]['tag']}")
        return deployed[model_name]["tag"]

    tag = generate_short_tag(model_name)
    deploy_cmd = [
        "emd", "deploy",
        "--model-id", model_name,
        "--instance-type", instance_type,
        "--engine-type", engine_type,
        "--service-type", "sagemaker_realtime",
        "--skip-confirm",
        "--model-tag", tag,
         "--extra-params", "{}"
    ]
    subprocess.run(deploy_cmd, check=True, env=os.environ)
    logging.info(f"ğŸš€ Deployment initiated for {model_name} with tag={tag}")
    return tag

def check_model_deployment(model_name):
    """check if the model to reach CREATE_COMPLETE status."""
    logging.info(f"â³ Waiting for {model_name} deployment to complete...")
    models = get_current_models()
    if model_name in models:
        logging.info(f"ğŸ‰ {model_name} deployed successfully with tag: {models[model_name]['tag']}")
        return True
    logging.info(f"ğŸ”„ {model_name} not yet deployed, please wait")
    return False

def wait_for_model_deployment(model_name, timeout=1800, check_interval=60):
    """Wait for the model to reach CREATE_COMPLETE status."""
    logging.info(f"â³ Waiting for {model_name} deployment to complete...")
    start_time = time.time()
    while time.time() - start_time < timeout:
        models = get_current_models()
        if model_name in models:
            logging.info(f"ğŸ‰ {model_name} deployed successfully with tag: {models[model_name]['tag']}")
            return True
        logging.info(f"ğŸ”„ {model_name} not yet deployed, checking again in {check_interval}s...")
        time.sleep(check_interval)
    logging.error(f"âŒ Timeout reached. {model_name} deployment did not complete within {timeout}s.")
    return False


# Default model tag - Update this for your deployment
DEFAULT_EMD_TAG = "test200"

# Dynamic model management - use functions instead of hardcoded dict

# Cache for EMD base URL to avoid repeated calls
_emd_base_url_cache = {"url": None, "timestamp": 0, "cache_duration": 300}  # 5 minutes cache

def set_emd_tag(tag):
    """Update the default EMD tag for all models"""
    global DEFAULT_EMD_TAG
    DEFAULT_EMD_TAG = tag
    logging.info(f"Updated EMD tag to: {tag}")

def get_emd_info():
    """Get current EMD configuration info"""
    return {
        "models": EMD_MODELS,
        "default_tag": DEFAULT_EMD_TAG,
        "base_url": get_emd_base_url()
    }

def get_emd_base_url(model_id, tag):
    status = get_model_status(model_id, tag)
    completed_status = status.get("completed")
    print("completed_status")
    if completed_status is None:
        return None
    outputs = completed_status[0].get("outputs")
    if outputs is None:
        return None
    try:
        outputs = json.loads(outputs.replace("'", "\""))
        BaseURL = outputs.get("BaseURL")
        return BaseURL
    except Exception as e:
        print("Error:", e)
        return None
    

def create_emd_openai_client(model_id, tag):
    """Create OpenAI client for EMD endpoints"""
    base_url = get_emd_base_url(model_id, tag)
    if base_url:
        return OpenAI(api_key="", base_url=f"{base_url}/v1")
    return None

def encode_image_for_emd(image_base64):
    """Encode image for EMD inference"""
    return image_base64

def get_account_id():
    """åŠ¨æ€è·å–å½“å‰AWSè´¦æˆ·ID"""
    try:
        sts_client = boto3.client('sts')
        identity = sts_client.get_caller_identity()
        return identity['Account']
    except Exception as e:
        logging.warning(f"æ— æ³•è·å–è´¦æˆ·IDï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return "651433607849"  # æ›´æ–°çš„è´¦æˆ·ID

def build_inference_profile_arn(model_type, account_id=None, region='us-west-2'):
    """æ„å»ºæ¨ç†é…ç½®æ–‡ä»¶ARNæˆ–è¿”å›æ ‡å‡†æ¨¡å‹ID"""
    # ä½¿ç”¨æ ‡å‡†æ¨¡å‹IDé¿å…è·¨è´¦æˆ·æƒé™é—®é¢˜
    model_mappings = {
        'claude4': 'anthropic.claude-3-5-sonnet-20241022-v2:0',  # ä½¿ç”¨æ ‡å‡†Claude 3.5æ¨¡å‹
        'nova': 'amazon.nova-pro-v1:0'
    }
    
    if model_type in model_mappings:
        return model_mappings[model_type]
    else:
        return model_type

def extract_frames_from_video(video_base64, num_frames=8):
    """ä»base64è§†é¢‘ä¸­æå–å…³é”®å¸§"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # ä¿å­˜base64è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            video_path = os.path.join(temp_dir, "input_video.mp4")
            with open(video_path, "wb") as f:
                f.write(base64.b64decode(video_base64))
            
            # è·å–è§†é¢‘æ—¶é•¿
            try:
                duration_cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 '{video_path}'"
                duration = float(subprocess.check_output(duration_cmd, shell=True, env=os.environ.copy()).decode().strip())
            except Exception as e:
                logging.warning(f"æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                duration = 10.0  # é»˜è®¤10ç§’
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            frames_dir = os.path.join(temp_dir, "frames")
            os.makedirs(frames_dir, exist_ok=True)
            
            # è®¡ç®—å¸§é—´éš”
            interval = max(duration / num_frames, 0.5)  # æœ€å°é—´éš”0.5ç§’
            
            frame_base64_list = []
            
            # æå–å…³é”®å¸§
            for i in range(num_frames):
                timestamp = min(i * interval, duration - 0.1)  # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘é•¿åº¦
                frame_path = os.path.join(frames_dir, f"frame_{i+1:02d}.jpg")
                
                # ä½¿ç”¨ffmpegæå–å¸§
                ffmpeg_cmd = f"ffmpeg -ss {timestamp} -i '{video_path}' -frames:v 1 -q:v 2 '{frame_path}' -y -loglevel quiet"
                
                try:
                    subprocess.run(ffmpeg_cmd, shell=True, check=True, env=os.environ.copy())
                    
                    # å°†å¸§è½¬æ¢ä¸ºbase64
                    if os.path.exists(frame_path):
                        with Image.open(frame_path) as img:
                            # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥èŠ‚çœtoken
                            img.thumbnail((800, 600), Image.Resampling.LANCZOS)
                            buffer = io.BytesIO()
                            img.save(buffer, format="JPEG", quality=85)
                            frame_base64 = base64.b64encode(buffer.getvalue()).decode()
                            frame_base64_list.append(frame_base64)
                            
                except subprocess.CalledProcessError as e:
                    logging.warning(f"æå–ç¬¬{i+1}å¸§å¤±è´¥: {e}")
                    continue
            
            logging.info(f"æˆåŠŸæå–äº† {len(frame_base64_list)} ä¸ªå…³é”®å¸§")
            return frame_base64_list
            
    except Exception as e:
        logging.error(f"è§†é¢‘åˆ‡å¸§å¤„ç†å¤±è´¥: {e}")
        return []

def enhance_prompt_for_video(original_prompt, num_frames):
    """ä¸ºè§†é¢‘åˆ†æå¢å¼ºæç¤ºè¯"""
    video_context = f"These are {num_frames} keyframes extracted from a video at regular intervals. Please analyze these sequential frames to understand the video content and answer the following question: "
    return video_context + original_prompt

def process_model_async(model_name, endpoint_func, data, result_queue):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ¨¡å‹çš„æ¨ç†"""
    start_time = time.time()
    try:
        result = endpoint_func(data)
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        
        result_queue.put({
            'model': model_name,
            'status': 'success',
            'result': result,
            'metadata': {
                'processingTime': processing_time,
                'startTime': start_time,
                'endTime': end_time
            }
        })
    except Exception as e:
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        
        result_queue.put({
            'model': model_name,
            'status': 'error',
            'error': str(e),
            'metadata': {
                'processingTime': processing_time,
                'startTime': start_time,
                'endTime': end_time
            }
        })


@app.route('/api/emd/init', methods=['POST'])
def api_init_emd():
    region = request.json.get('region', 'us-west-2')
    try:
        init_emd_env(region=region)
        return jsonify({"status": "success", "region": region})
    except Exception as e:
        logging.error(f"EMD Init Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500
    
@app.route('/api/emd/current-models', methods=['GET'])
def api_get_current_models():
    try:
        models = get_current_models()
        return jsonify({"status": "success", "deployed_models": models})
    except Exception as e:
        logging.error(f"Get Current Models Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

    
@app.route('/api/emd/check-deployment', methods=['GET'])
def api_check_model_deployment():
    model_name = request.args.get('model_name')
    if not model_name:
        return jsonify({"status": "error", "message": "model_name is required"}), 400

    try:
        if check_model_deployment(model_name):
            return jsonify({"status": "deployed", "model_name": model_name})
        else:
            return jsonify({"status": "deploying", "model_name": model_name, "message": "æ¨¡å‹ä»åœ¨éƒ¨ç½²ä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"})
    except Exception as e:
        logging.error(f"Check Model Deployment Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500
    
    
@app.route('/api/multi-inference', methods=['POST'])
def multi_inference():
    """åŒæ—¶è°ƒç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒæµå¼è¿”å›ç»“æœ"""
    data = request.json
    models = data.get('models', BEDROCK_MODELS)
    
    def generate():
        result_queue = queue.Queue()
        threads = []
        
        # åˆ›å»ºçº¿ç¨‹å¤„ç†æ¯ä¸ªæ¨¡å‹
        for model in models:
            if model == 'claude4':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_claude4_internal(d), data, result_queue)
                )
            elif model == 'claude35':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_claude35_internal(d), data, result_queue)
                )
            elif model == 'nova':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_nova_internal(d), data, result_queue)
                )
            elif model in EMD_MODELS:
                # EMD model - check if deployed first
                deployed_models = get_current_models()["deployed"]
                print("[debug] deployed_models", deployed_models, model)
                if model in deployed_models:
                    thread = threading.Thread(
                        target=process_model_async,
                        args=(model, lambda d: call_emd_model_internal(d, model), data, result_queue)
                    )
                else:
                    # Model not deployed, return waiting message
                    result_queue.put({
                        'model': model,
                        'status': 'not_deployed',
                        'message': f'æ¨¡å‹ {model} æ­£åœ¨éƒ¨ç½²ä¸­æˆ–å°šæœªéƒ¨ç½²ï¼Œè¯·ç¨åå†è¯•ã€‚'
                    })
                    continue
            else:
                continue
                
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…ç»“æœå¹¶æµå¼è¿”å›
        completed = 0
        total_models = len(threads)
        
        while completed < total_models:
            try:
                result = result_queue.get(timeout=1)
                completed += 1
                
                # å‘é€ç»“æœ
                yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
                
            except queue.Empty:
                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        yield f"data: {json.dumps({'type': 'complete'})}\n\n"
    
    return Response(
        generate(),
        mimetype='text/plain',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*'
        }
    )

def call_claude4_internal(data):
    """Claude4æ¨ç†çš„å†…éƒ¨å‡½æ•°"""
    start_time = time.time()
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        content = [{"type": "text", "text": text}]
        
        # å¤„ç†è§†é¢‘å’Œå›¾ç‰‡
        if media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§
            if len(frames) > 0:
                video_base64 = frames[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"type": "text", "text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": frame_base64
                            }
                        })
                    logging.info(f"æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    return jsonify({"error": "Failed to extract frames from video. Please check the video format."}), 400
            else:
                return jsonify({"error": "No video data received."}), 400
        else:  # å¤„ç†å›¾ç‰‡
            for img_base64 in frames:
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": img_base64
                    }
                })
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        # åŠ¨æ€æ„å»ºClaude 4æ¨ç†é…ç½®æ–‡ä»¶ARN
        model_id = build_inference_profile_arn('claude4')
        logging.info(f"Using Claude 4 model ID: {model_id}")
        
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        
        # æ·»åŠ å¤„ç†æ—¶é—´å…ƒæ•°æ®
        response_with_metadata = {
            **response_body,
            'metadata': {
                'processingTime': processing_time,
                'startTime': start_time,
                'endTime': end_time
            }
        }
        
        log_entry['response'] = response_body
        log_entry['processing_time'] = processing_time
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return response_with_metadata
    except Exception as e:
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        log_entry['error'] = str(e)
        log_entry['processing_time'] = processing_time
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

@app.route('/api/claude4', methods=['POST'])
def call_claude4():
    try:
        result = call_claude4_internal(request.json)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def call_claude35_internal(data):
    """Claude3.5æ¨ç†çš„å†…éƒ¨å‡½æ•°"""
    start_time = time.time()
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        content = [{"type": "text", "text": text}]
        
        # å¤„ç†è§†é¢‘å’Œå›¾ç‰‡
        if media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§
            if len(frames) > 0:
                video_base64 = frames[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"type": "text", "text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": frame_base64
                            }
                        })
                    logging.info(f"æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    return jsonify({"error": "Failed to extract frames from video. Please check the video format."}), 400
            else:
                return jsonify({"error": "No video data received."}), 400
        else:  # å¤„ç†å›¾ç‰‡
            for img_base64 in frames:
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": img_base64
                    }
                })
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        response = bedrock_client.invoke_model(
            modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        
        # æ·»åŠ å¤„ç†æ—¶é—´å…ƒæ•°æ®
        response_with_metadata = {
            **response_body,
            'metadata': {
                'processingTime': processing_time,
                'startTime': start_time,
                'endTime': end_time
            }
        }
        
        log_entry['response'] = response_body
        log_entry['processing_time'] = processing_time
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return response_with_metadata
    except Exception as e:
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        log_entry['error'] = str(e)
        log_entry['processing_time'] = processing_time
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

@app.route('/api/claude35', methods=['POST'])
def call_claude35():
    try:
        result = call_claude35_internal(request.json)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def call_nova_internal(data):
    """Novaæ¨ç†çš„å†…éƒ¨å‡½æ•°"""
    start_time = time.time()
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        import base64
        # Nova Pro model supports both images and videos
        if media_type == 'image':
            content = [{"text": text}]
            for img_base64 in frames:
                content.append({
                    "image": {
                        "format": "jpeg",
                        "source": {"bytes": img_base64}
                    }
                })
        elif media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§å¹¶è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼
            if len(frames) > 0:
                video_base64 = frames[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("Novaå¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "image": {
                                "format": "jpeg",
                                "source": {"bytes": frame_base64}
                            }
                        })
                    logging.info(f"NovaæˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    raise Exception("Failed to extract frames from video for Nova processing.")
            else:
                raise Exception("No video data received for Nova.")
        else:
            raise Exception("Unsupported media type")
        
        request_body = {
            "schemaVersion": "messages-v1",
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": max_tokens,
                "temperature": temperature
            }
        }
        
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        
        # åŠ¨æ€æ„å»ºNovaæ¨ç†é…ç½®æ–‡ä»¶ARN
        model_id = build_inference_profile_arn('nova')
        logging.info(f"Using Nova model ID: {model_id}")
        
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        
        response_body = json.loads(response['body'].read())
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        
        # æ·»åŠ å¤„ç†æ—¶é—´å…ƒæ•°æ®
        response_with_metadata = {
            **response_body,
            'metadata': {
                'processingTime': processing_time,
                'startTime': start_time,
                'endTime': end_time
            }
        }
        
        log_entry['response'] = response_body
        log_entry['processing_time'] = processing_time
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return response_with_metadata
        
    except Exception as e:
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        log_entry['error'] = str(e)
        log_entry['processing_time'] = processing_time
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

@app.route('/api/nova', methods=['POST'])
def call_nova():
    try:
        # Convert media to frames format for internal function
        data = request.json.copy()
        if 'media' in data:
            data['frames'] = data['media']
        result = call_nova_internal(data)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def call_emd_model_internal(data, model_key):
    """EMD model inference internal function"""
    start_time = time.time()
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    if model_key not in EMD_MODELS:
        raise Exception(f"Unsupported EMD model: {model_key}")
    
    # Check if model is actually deployed using dynamic function
    deployed_models = get_current_models()["deployed"]
    if model_key not in deployed_models:
        available_models = list(deployed_models.keys())
        raise Exception(f"Model {model_key} is not currently deployed. Available models: {available_models}. Please deploy this model first.")
    
    model_id = EMD_MODELS[model_key]["model_path"]
    deployed_tag = deployed_models[model_key]['tag']
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature, 'model': model_id}
    }
    
    try:
        logging.info(f"ğŸš€ Starting EMD inference for model {model_id}")
        logging.info(f"âœ… Using deployed model {model_key} with tag {deployed_tag}")
        
        # Update the default tag to the deployed one
        # original_tag = DEFAULT_EMD_TAG
        # set_emd_tag(deployed_tag)
        
        # First check deployment status
        # deployment_info = deployment_status.get(model_key)
        # if deployment_info:
        #     if deployment_info["status"] == "deploying":
        #         error_msg = f"æ¨¡å‹ {model_key} æ­£åœ¨éƒ¨ç½²ä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚éƒ¨ç½²çŠ¶æ€: {deployment_info['message']}"
        #         raise Exception(error_msg)
        #     elif deployment_info["status"] == "failed":
        #         # For failed deployments, use the known deployed model tag
        #         logging.warning(f"âš ï¸ New deployment failed for {model_key}, using existing deployment with tag {deployed_tag}")
        #     elif deployment_info["status"] == "deployed":
        #         # Use the newly deployed model's tag
        #         newly_deployed_tag = deployment_info["tag"]
        #         logging.info(f"âœ… Using newly deployed model {model_key} with tag {newly_deployed_tag}")
        #         set_emd_tag(newly_deployed_tag)
        #         deployed_tag = newly_deployed_tag
        
        # Try OpenAI client first (API endpoint)
        logging.info(f"ğŸ” Checking EMD OpenAI client availability...")
        # client = OpenAI(
        #     api_key="",
        #     base_url=base_url
        # )
        openai_client = create_emd_openai_client(model_id, deployed_tag)
        print("openai_client", openai_client)
        if openai_client:
            logging.info(f"âœ… EMD OpenAI client available, proceeding with inference")
            result = call_emd_via_openai(openai_client, data, model_id, deployed_tag, model_key, log_entry)
            end_time = time.time()
            processing_time = f"{end_time - start_time:.2f}s"
            
            # æ·»åŠ å¤„ç†æ—¶é—´å…ƒæ•°æ®
            if isinstance(result, dict):
                result['metadata'] = {
                    'processingTime': processing_time,
                    'startTime': start_time,
                    'endTime': end_time
                }
            return result
        
        # Fallback to SageMaker client
        # logging.info(f"ğŸ” Checking EMD SageMaker client availability...")
        # sagemaker_client = create_emd_sagemaker_client(model_id)
        # sagemaker_client = SageMakerClient(
        #     model_id=model_id,
        #     model_tag=deployed_tag
        # )
        # print("model_id", model_id, "deployed_tag", deployed_tag)
        # if sagemaker_client:
        #     logging.info(f"âœ… EMD SageMaker client available, proceeding with inference")
        #     result = call_emd_via_sagemaker(sagemaker_client, data, model_id, model_key, log_entry)
        #     end_time = time.time()
        #     processing_time = f"{end_time - start_time:.2f}s"
            
        #     # æ·»åŠ å¤„ç†æ—¶é—´å…ƒæ•°æ®
        #     if isinstance(result, dict):
        #         result['metadata'] = {
        #             'processingTime': processing_time,
        #             'startTime': start_time,
        #             'endTime': end_time
        #         }
        #     return result
        
        # No client available - provide helpful error message
        logging.error(f"âŒ No EMD client available for {model_id}")
        error_msg = f"æ¨¡å‹ {model_key} çš„éƒ¨ç½²ä¸å¯ç”¨ã€‚å½“å‰å¯ç”¨æ¨¡å‹: {list(deployed_models.keys())}ã€‚"
        raise Exception(error_msg)
        
    except Exception as e:
        end_time = time.time()
        processing_time = f"{end_time - start_time:.2f}s"
        log_entry['error'] = str(e)
        log_entry['processing_time'] = processing_time
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

def call_emd_via_openai(client, data, model_id, tag, model_key, log_entry):
    """Call EMD model via OpenAI API - Updated with working pattern"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    logging.info(f"ğŸ¯ Processing {media_type} input for EMD model {model_id}")
    logging.info(f"ğŸ“Š Request details: text_len={len(text)}, frames={len(frames)}, type={media_type}")
    
    # For Qwen2.5-0.5B-Instruct, prioritize SageMaker client over OpenAI
    # if model_id == 'Qwen2.5-0.5B-Instruct':
    #     logging.info(f"ğŸ”„ Switching to SageMaker client for {model_id} (more reliable)")
    #     try:
    #         sagemaker_client = create_emd_sagemaker_client(model_id)
    #         if sagemaker_client:
    #             return call_emd_via_sagemaker(sagemaker_client, data, model_id, model_key, log_entry)
    #     except Exception as e:
    #         logging.warning(f"âš ï¸ SageMaker fallback failed, trying OpenAI: {e}")
    
    messages = []
    
    # Check if this is a text-only model (like Qwen2.5-0.5B-Instruct)
    if model_id == 'Qwen2.5-0.5B-Instruct':
        # Text-only model - just send the text prompt
        logging.info(f"ğŸ“ Processing text-only input for EMD model {model_id}")
        messages.append({
            "role": "user",
            "content": text
        })
    elif media_type == 'video':
        # Process video: extract keyframes
        if len(frames) > 0:
            video_base64 = frames[0]
            logging.info(f"ğŸ“¹ EMD {model_id} å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
            keyframes = extract_frames_from_video(video_base64, num_frames=8)
            
            if keyframes:
                enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                content = [{"type": "text", "text": enhanced_text}]
                
                for frame_base64 in keyframes:
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame_base64}"
                        }
                    })
                
                messages.append({
                    "role": "user",
                    "content": content
                })
                logging.info(f"âœ… EMD {model_id} æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
            else:
                raise Exception("Failed to extract frames from video for EMD processing")
        else:
            raise Exception("No video data received for EMD")
    else:
        # Process images (for multimodal models)
        logging.info(f"ğŸ–¼ï¸ Processing {len(frames)} images for EMD model {model_id}")
        content = [{"type": "text", "text": text}]
        for img_base64 in frames:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{img_base64}"
                }
            })
        
        messages.append({
            "role": "user",
            "content": content
        })
    
    # Use the configured model tag
    model_endpoint = f"{model_id}/{tag}"
    logging.info(f"ğŸ”¥ Calling EMD OpenAI API for {model_endpoint}...")
    logging.info(f"ğŸŒ Using base URL: {client.base_url}")
    logging.info(f"ğŸ“ Message structure: {len(messages)} messages")
    
    try:
        # base_url = "http://EMD-EC-Publi-nOXevnWhCba1-203402465.us-west-2.elb.amazonaws.com/v1"

        # client = OpenAI(
        #     api_key="",
        #     base_url=base_url
        # )
        # è¿™éƒ¨åˆ†ç›®å‰æœ‰bugï¼Œå¦‚æœä»æœ¬åœ°è¯»å…¥å›¾ç‰‡æ˜¯å¯ä»¥æ­£ç¡®é¢„æµ‹çš„ï¼Œä½†æ˜¯ä»å‰ç«¯ä¼ å…¥çš„å›¾ç‰‡ä¼¼ä¹è§£ææœ‰é—®é¢˜
        # model_endpoint = "Qwen2-VL-7B-Instruct/dev"
        response = client.chat.completions.create(
            model=model_endpoint,
            messages=messages,
            # max_tokens=max_tokens,
            # temperature=temperature
        )

        # base_url = "http://EMD-EC-Publi-nOXevnWhCba1-203402465.us-west-2.elb.amazonaws.com/v1"

        # client = OpenAI(
        #     api_key="",
        #     base_url=base_url
        # )

        # image_path = "/home/ec2-user/efs_data/workspace/multimodal-platform/image.jpg"
        # base64_image = encode_image(image_path)
        # print('base64_image=', base64_image[:10])

        # response = client.chat.completions.create(
        #     model="Qwen2-VL-7B-Instruct/dev",  # Vision model ID with tag
        #     messages=[
        #         {
        #             "role": "user",
        #             "content": [
        #                 {"type": "text", "text": "What's in this image?"},
        #                 {
        #                     "type": "image_url",
        #                     "image_url": {
        #                         "url": f"data:image/jpeg;base64,{base64_image}"
        #                     }
        #                 }
        #             ]
        #         }
        #     ]
        # )
    except Exception as e:
        error_str = str(e).lower()
        logging.error(f"âŒ EMD OpenAI API call failed: {str(e)}")
        logging.error(f"ğŸ” Model endpoint used: {model_endpoint}")
        logging.error(f"ğŸŒ Base URL used: {client.base_url}")
        
        # Check if it's a deployment/startup issue
        if "404" in error_str or "not found" in error_str or "service unavailable" in error_str:
            # For deployed models that are detected but endpoints are not ready
            deployed_models = get_current_models()
            if model_key in deployed_models:
                raise Exception(f"æ¨¡å‹ {model_key} æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ã€‚SageMaker ç«¯ç‚¹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´å®Œå…¨å¯åŠ¨ã€‚")
            else:
                raise Exception(f"æ¨¡å‹ {model_key} ç«¯ç‚¹ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥éƒ¨ç½²çŠ¶æ€æˆ–é‡æ–°éƒ¨ç½²æ¨¡å‹ã€‚")
        else:
            raise
    
    logging.info(f"âœ… EMD OpenAI API call completed for {model_id}")
    log_entry['response'] = response.model_dump()
    logging.info(json.dumps(log_entry, ensure_ascii=False))
    
    # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼ŒåŒ¹é…Bedrockæ¨¡å‹æ ¼å¼
    return {
        "content": [{"type": "text", "text": response.choices[0].message.content}],
        "usage": response.usage.model_dump() if response.usage else None
    }

def call_emd_via_sagemaker(client, data, model_id, model_key, log_entry):
    """Call EMD model via SageMaker SDK"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    logging.info(f"ğŸ¯ Processing {media_type} input for EMD SageMaker model {model_id}")
    messages = []
    
    # Check if this is a text-only model (like Qwen2.5-0.5B-Instruct)
    if model_id == 'Qwen2.5-0.5B-Instruct':
        # Text-only model - just send the text prompt
        logging.info(f"ğŸ“ Processing text-only input for EMD SageMaker model {model_id}")
        messages.append({
            "role": "user",
            "content": text
        })
    elif media_type == 'video':
        # Process video: extract keyframes
        if len(frames) > 0:
            video_base64 = frames[0]
            logging.info(f"ğŸ“¹ EMD SageMaker {model_id} å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
            keyframes = extract_frames_from_video(video_base64, num_frames=8)
            
            if keyframes:
                enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                content = [{"type": "text", "text": enhanced_text}]
                
                for frame_base64 in keyframes:
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame_base64}"
                        }
                    })
                
                messages.append({
                    "role": "user",
                    "content": content
                })
                logging.info(f"âœ… EMD SageMaker {model_id} æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
            else:
                raise Exception("Failed to extract frames from video for EMD SageMaker processing")
        else:
            raise Exception("No video data received for EMD SageMaker")
    else:
        # Process images (for multimodal models)
        logging.info(f"ğŸ–¼ï¸ Processing {len(frames)} images for EMD SageMaker model {model_id}")
        content = [{"type": "text", "text": text}]
        # print("[debug] text", text, "frames", frames)
        # for img_base64 in frames:
        #     content.append({
        #         "type": "image_url",
        #         "image_url": {
        #             "url": f"data:image/jpeg;base64,{img_base64}"
        #         }
        #     })
        
        messages.append({
            "role": "user",
            "content": content
        })
    
    logging.info(f"ğŸ”¥ Calling EMD SageMaker SDK for {model_id}...")
    
    try:
        # client = SageMakerClient(
        #     model_id="Qwen2-VL-7B-Instruct",
        #     model_tag="dev"
        # )
        # messages = {
        #     "messages": [
        #         {
        #             "role": "user",
        #             "content": [
        #                 {"type": "text", "text": "Who are you?"},
        #             ]
        #         }
        #     ]
        # }
        # print(messages)
        # # response = client.invoke({"messages": messages})
        # response = client.invoke(messages)

        base_url = "http://EMD-EC-Publi-nOXevnWhCba1-203402465.us-west-2.elb.amazonaws.com/v1"

        client = OpenAI(
            api_key="",
            base_url=base_url
        )

        # image_path = "./image.jpg"
        # base64_image = encode_image(image_path)
        # print('base64_image=', base64_image[:10])

        response = client.chat.completions.create(
            model="Qwen2-VL-7B-Instruct/dev",  # Vision model ID with tag
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "What's in this image?"},
                        # {
                        #     "type": "image_url",
                        #     "image_url": {
                        #         "url": f"data:image/jpeg;base64,{base64_image}"
                        #     }
                        # }
                    ]
                }
            ]
        )
    except Exception as e:
        error_str = str(e).lower()
        logging.error(f"âŒ EMD SageMaker SDK call failed: {str(e)}")
        
        # Check if it's a deployment/startup issue
        if "404" in error_str or "not found" in error_str or "service unavailable" in error_str or "model error" in error_str:
            # For deployed models that are detected but endpoints are not ready
            deployed_models = get_current_models()
            if model_key in deployed_models:
                raise Exception(f"æ¨¡å‹ {model_key} æ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•ã€‚SageMaker ç«¯ç‚¹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´å®Œå…¨å¯åŠ¨ã€‚")
            else:
                raise Exception(f"æ¨¡å‹ {model_key} ç«¯ç‚¹ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥éƒ¨ç½²çŠ¶æ€æˆ–é‡æ–°éƒ¨ç½²æ¨¡å‹ã€‚")
        else:
            raise
    
    logging.info(f"âœ… EMD SageMaker SDK call completed for {model_id}")
    log_entry['response'] = response.model_dump()
    logging.info(json.dumps(log_entry, ensure_ascii=False))
    
    # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼Œç¡®ä¿å’Œå…¶ä»–æ¨¡å‹æ ¼å¼ä¸€è‡´
    # if isinstance(response, dict) and 'choices' in response:
    #     # å¦‚æœresponseå·²ç»æœ‰æ­£ç¡®æ ¼å¼ï¼Œç¡®ä¿contentæœ‰typeå­—æ®µ
    #     if isinstance(response['choices'], list) and len(response['choices']) > 0:
    #         for item in response['content']:
    #             if isinstance(item, dict) and 'text' in item and 'type' not in item:
    #                 item['type'] = 'text'
    #     return response
    # else:
    #     # å¦‚æœresponseæ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•æå–æ–‡æœ¬å†…å®¹
    #     text_content = str(response) if response else "No response"
    #     return {
    #         "content": [{"type": "text", "text": text_content}],
    #         "usage": None
    #     }

    # return {
    #     "content": [{"type": "text", "text": response["choices"][0]["message"]["content"].strip()}],
    #     "usage": None
    # }

    return {
        "content": [{"type": "text", "text": response.choices[0].message.content.strip()}],
        "usage": None
    }

# EMD model endpoints
@app.route('/api/emd/<model_key>', methods=['POST'])
def call_emd_model(model_key):
    """Generic EMD model endpoint"""
    try:
        result = call_emd_model_internal(request.json, model_key)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/emd/status', methods=['GET'])
def emd_status():
    """Get EMD deployment status"""
    try:
        # Ensure subprocess inherits AWS environment variables
        env = os.environ.copy()
        logging.info("Getting EMD status...")
        result = subprocess.run(['emd', 'status'], capture_output=True, text=True, env=env, timeout=30)
        if result.returncode == 0:
            logging.info("âœ… EMD status retrieved successfully")
            return jsonify({"status": "success", "output": result.stdout})
        else:
            logging.warning(f"âŒ EMD status failed: {result.stderr}")
            # Check if it's a token issue
            if "InvalidClientTokenId" in result.stderr or "security token" in result.stderr.lower():
                return jsonify({
                    "status": "auth_error", 
                    "message": "AWS credentials expired. Please refresh your AWS session.",
                    "output": result.stderr
                }), 401
            else:
                return jsonify({"status": "error", "output": result.stderr}), 500
    except subprocess.TimeoutExpired:
        logging.error("EMD status command timed out")
        return jsonify({"error": "EMD status command timed out"}), 408
    except Exception as e:
        logging.error(f"EMD status exception: {e}")
        return jsonify({"error": str(e)}), 500

# @app.route('/api/emd/models', methods=['GET'])
# def emd_models():
#     """Get available EMD models"""
#     return jsonify({
#         "all_models": EMD_MODELS,
#         "deployed_models": get_current_models(),
#         "available_for_inference": list(DEPLOYED_MODELS.keys())
#     })

@app.route('/api/model-list', methods=['GET'])
def get_model_list():
    """è·å–æ‰€æœ‰æ¨¡å‹åˆ—è¡¨ä¿¡æ¯"""
    try:
        logging.info("[DEBUG] æ¥æ”¶åˆ° /api/model-list è¯·æ±‚")
        logging.info(f"[DEBUG] ALL_MODELS ç»“æ„: {ALL_MODELS}")
        logging.info(f"[DEBUG] EMD_MODELS keys: {list(EMD_MODELS.keys())}")
        logging.info(f"[DEBUG] BEDROCK_MODELS keys: {list(BEDROCK_MODELS.keys())}")
        
        response_data = {
            "status": "success",
            "models": ALL_MODELS
        }
        logging.info(f"[DEBUG] è¿”å›çš„æ¨¡å‹åˆ—è¡¨æ•°æ®: {response_data}")
        return jsonify(response_data)
    except Exception as e:
        logging.error(f"[DEBUG] è·å–æ¨¡å‹åˆ—è¡¨ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/api/emd/config', methods=['GET'])
def emd_config():
    """Get EMD configuration information"""
    return jsonify(get_emd_info())

@app.route('/api/emd/config/tag', methods=['POST'])
def update_emd_tag():
    """Update the default EMD tag"""
    try:
        data = request.json
        new_tag = data.get('tag')
        if new_tag:
            set_emd_tag(new_tag)
            return jsonify({"status": "success", "new_tag": new_tag})
        else:
            return jsonify({"error": "Tag is required"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Global deployment tracking
deployment_status = {}
deployment_threads = {}
bootstrap_status = {"status": "unknown", "last_check": 0}

def check_emd_bootstrap_status():
    """Check if EMD environment is bootstrapped"""
    global bootstrap_status
    
    # Cache bootstrap status for 5 minutes
    current_time = time.time()
    if current_time - bootstrap_status["last_check"] < 300:  # 5 minutes cache
        return bootstrap_status["status"] == "ready"
    
    try:
        env = os.environ.copy()
        result = subprocess.run(['emd', 'status'], capture_output=True, text=True, env=env, timeout=30)
        
        if result.returncode == 0:
            bootstrap_status = {"status": "ready", "last_check": current_time}
            logging.info("âœ… EMD environment is bootstrapped and ready")
            return True
        else:
            if "stack" in result.stderr.lower() or "environment" in result.stderr.lower():
                bootstrap_status = {"status": "not_bootstrapped", "last_check": current_time}
                logging.warning("âš ï¸ EMD environment needs bootstrapping")
                return False
            else:
                bootstrap_status = {"status": "error", "last_check": current_time}
                logging.error(f"âŒ EMD status check failed: {result.stderr}")
                return False
    except Exception as e:
        bootstrap_status = {"status": "error", "last_check": current_time}
        logging.error(f"âŒ EMD bootstrap check failed: {e}")
        return False

def run_emd_bootstrap():
    """Run EMD bootstrap command"""
    try:
        logging.info("ğŸš€ Starting EMD bootstrap...")
        env = os.environ.copy()
        result = subprocess.run(['emd', 'bootstrap'], capture_output=True, text=True, env=env, timeout=600)  # 10 min timeout
        
        if result.returncode == 0:
            logging.info("âœ… EMD bootstrap completed successfully")
            bootstrap_status["status"] = "ready"
            bootstrap_status["last_check"] = time.time()
            return True, result.stdout
        else:
            logging.error(f"âŒ EMD bootstrap failed: {result.stderr}")
            return False, result.stderr
    except subprocess.TimeoutExpired:
        logging.error("âŒ EMD bootstrap timed out")
        return False, "Bootstrap command timed out"
    except Exception as e:
        logging.error(f"âŒ EMD bootstrap exception: {e}")
        return False, str(e)

def generate_deployment_tag(model_name):
    """Generate a unique deployment tag for the model"""
    timestamp = datetime.now().strftime("%m%d%H%M")
    clean_name = model_name.lower().replace('.', '').replace('-', '')
    return f"{timestamp}"

def deploy_emd_model_background(model_name, tag, config=None):
    """Deploy EMD model in background thread with deployment configuration"""
    global deployment_status
    
    # Extract configuration parameters
    if config is None:
        config = {}
    
    framework = config.get('framework', 'vllm')
    machine_type = config.get('machineType', 'g5.2xlarge')
    tp_size = config.get('tpSize', 1)
    dp_size = config.get('dpSize', 1)
    
    logging.info(f"EMD background deployment: {model_name}, framework={framework}, machine_type={machine_type}, tp_size={tp_size}, dp_size={dp_size}")
    
    try:
        # Step 1: Check bootstrap status
        deployment_status[model_name] = {
            "status": "checking_bootstrap",
            "tag": tag,
            "message": f"Checking EMD environment for {model_name}...",
            "start_time": datetime.now().isoformat()
        }
        
        # Check if EMD is bootstrapped
        if not check_emd_bootstrap_status():
            deployment_status[model_name]["status"] = "bootstrapping"
            deployment_status[model_name]["message"] = f"Bootstrapping EMD environment for {model_name}..."
            
            success, output = run_emd_bootstrap()
            if not success:
                deployment_status[model_name] = {
                    "status": "failed",
                    "tag": tag,
                    "message": f"Bootstrap failed for {model_name}",
                    "error": output,
                    "end_time": datetime.now().isoformat()
                }
                logging.error(f"âŒ Bootstrap failed for {model_name}: {output}")
                return
        
        # Step 2: Start deployment
        deployment_status[model_name] = {
            "status": "deploying",
            "tag": tag,
            "message": f"Deploying {model_name} with tag {tag}...",
            "start_time": datetime.now().isoformat()
        }
        
        # Get model ID from EMD_MODELS mapping
        model_id = EMD_MODELS.get(model_name)["model_path"]
        if not model_id:
            raise ValueError(f"Unknown model: {model_name}")
        
        # Use configured instance type, with model-based fallback
        instance_type = machine_type  # Use the configured machine type
        if not instance_type or instance_type == 'auto':
            # Fallback to model-based instance type selection
            instance_type = "g5.2xlarge"  # Default for smaller models
            if "vl-32b" in model_name:
                instance_type = "g5.12xlarge"  # Larger instance for 32B model
            elif "vl-7b" in model_name or "ui-tars" in model_name:
                instance_type = "g5.4xlarge"  # Medium instance for 7B models
        
        logging.info(f"Using instance type: {instance_type} for model {model_name}")
        logging.info(f"Using framework: {framework} for model {model_name}")
        
        # Build EMD deploy command with configured parameters
        extra_params = {
            'tp_size': tp_size,
            'dp_size': dp_size
        }
        
        deploy_cmd = [
            'emd', 'deploy',
            '--model-id', model_id,
            '--instance-type', instance_type,
            '--engine-type', framework,  # Use configured framework
            '--service-type', 'sagemaker_realtime',
            '--model-tag', tag,
            '--extra-params', json.dumps(extra_params),
            '--skip-confirm'
        ]

        logging.info(f"ğŸš€ Starting EMD deployment: model_id={model_id}, instance_type={instance_type}, engine={framework}, tag={tag}")
        logging.info(f"ğŸ“‹ Extra parameters: {extra_params}")
        result = deploy(
            model_id=model_id,
            instance_type=instance_type,
            engine_type=framework,  # Use configured framework
            service_type="sagemaker_realtime",
            model_tag=tag,
        )
        
        # Execute deployment command with inherited environment
        env = os.environ.copy()
        result = subprocess.run(deploy_cmd, capture_output=True, text=True, timeout=1800, env=env)  # 30 min timeout
        
        deployment_status[model_name] = {
            "status": "deployed",
            "tag": tag,
            "message": f"Successfully deployed {model_name}",
            "output": result.stdout,
            "end_time": datetime.now().isoformat()
        }
        # Deployment successful - models will be detected by get_current_models()
        logging.info(f"âœ… EMD deployment successful for {model_name} with tag {tag}")
        logging.info(f"ğŸ“ Model {model_name} is now available for inference")
        # else:
        #     deployment_status[model_name] = {
        #         "status": "failed",
        #         "tag": tag,
        #         "message": f"Deployment failed for {model_name}",
        #         "error": result.stderr,
        #         "output": result.stdout,
        #         "end_time": datetime.now().isoformat()
        #     }
        #     logging.error(f"âŒ EMD deployment failed for {model_name}: {result.stderr}")
            
    except Exception as e:
        deployment_status[model_name] = {
            "status": "failed",
            "tag": tag,
            "message": f"Deployment error for {model_name}: {str(e)}",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        }
        logging.error(f"âŒ EMD deployment exception for {model_name}: {e}")

##################
### flask api  ###
##################

@app.route('/api/deploy-models', methods=['POST'])
def deploy_models():
    """Deploy models with deployment configuration"""
    global deployment_threads
    
    try:
        data = request.json
        models = data.get('models', [])
        config = data.get('config', {})
        
        if not models:
            return jsonify({"error": "No models specified"}), 400
        
        # Extract deployment configuration
        deployment_method = config.get('method', 'SageMaker Endpoint')
        framework = config.get('framework', 'vllm')
        machine_type = config.get('machineType', 'g5.2xlarge')
        tp_size = config.get('tpSize', 1)
        dp_size = config.get('dpSize', 1)
        
        logging.info(f"Deployment request: models={models}, method={deployment_method}, framework={framework}, machine_type={machine_type}")
        
        # For now, only process the first model (single deployment)
        # TODO: Support batch deployment later
        model_name = models[0] if models else None
        
        if not model_name:
            return jsonify({"error": "No valid model specified"}), 400
        
        deployment_info = {}
        
        # Check deployment method
        if deployment_method == 'SageMaker Endpoint':
            # Use EMD deployment for SageMaker Endpoint
            if model_name not in EMD_MODELS:
                return jsonify({"error": f"Model {model_name} not supported for SageMaker Endpoint deployment"}), 400
                
            # Generate unique tag for this deployment
            tag = generate_deployment_tag(model_name)

            logging.info(f"Starting SageMaker Endpoint deployment: {model_name} with tag {tag}")
            
            # Check if already deploying
            if model_name in deployment_threads and deployment_threads[model_name].is_alive():
                deployment_info[model_name] = {
                    "status": "already_deploying",
                    "message": f"{model_name} is already being deployed"
                }
            else:
                # Start deployment thread with configuration
                thread = threading.Thread(
                    target=deploy_emd_model_background,
                    args=(model_name, tag, config),
                    daemon=True
                )
                thread.start()
                deployment_threads[model_name] = thread
                
                deployment_info[model_name] = {
                    "status": "started",
                    "tag": tag,
                    "message": f"Started SageMaker Endpoint deployment of {model_name}",
                    "config": {
                        "method": deployment_method,
                        "framework": framework,
                        "machine_type": machine_type,
                        "tp_size": tp_size,
                        "dp_size": dp_size
                    }
                }
                # Update global EMD tag to use the newest one
                set_emd_tag(tag)
                
        elif deployment_method == 'SageMaker HyperPod':
            # TODO: Implement SageMaker HyperPod deployment
            deployment_info[model_name] = {
                "status": "not_implemented",
                "message": f"SageMaker HyperPod deployment for {model_name} is not yet implemented",
                "config": config
            }
            logging.warning(f"SageMaker HyperPod deployment not implemented for {model_name}")
            
        elif deployment_method == 'EKS':
            # TODO: Implement EKS deployment
            deployment_info[model_name] = {
                "status": "not_implemented",
                "message": f"EKS deployment for {model_name} is not yet implemented",
                "config": config
            }
            logging.warning(f"EKS deployment not implemented for {model_name}")
            
        elif deployment_method == 'EC2':
            # TODO: Implement EC2 deployment
            deployment_info[model_name] = {
                "status": "not_implemented",
                "message": f"EC2 deployment for {model_name} is not yet implemented",
                "config": config
            }
            logging.warning(f"EC2 deployment not implemented for {model_name}")
            
        else:
            return jsonify({"error": f"Unsupported deployment method: {deployment_method}"}), 400
        
        return jsonify({
            "status": "success",
            "deployments": deployment_info,
            "deployment_method": deployment_method,
            "message": f"Started deployment for {len(deployment_info)} models using {deployment_method}"
        })
        
    except Exception as e:
        logging.error(f"Deploy models error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/check-model-status', methods=['POST'])
def check_model_status():
    """Check model deployment status for selected models"""
    data = request.json
    logging.info(f"[DEBUG] æ¥æ”¶åˆ° /api/check-model-status è¯·æ±‚, data: {data}")
    
    selected_models = data.get('models', [])
    logging.info(f"[DEBUG] é€‰ä¸­çš„æ¨¡å‹: {selected_models}, ç±»å‹: {type(selected_models)}")
    
    # æ£€æŸ¥è¯·æ±‚æ•°æ®çš„å®Œæ•´æ€§
    request_headers = {k: v for k, v in request.headers.items()}
    logging.info(f"[DEBUG] è¯·æ±‚å¤´: {request_headers}")
    logging.info(f"[DEBUG] è¯·æ±‚æ•°æ®: {request.get_data()}")    
    
    try:
        # Get currently deployed models
        current_models = get_current_models()
        logging.info(f"[DEBUG] å½“å‰éƒ¨ç½²çš„æ¨¡å‹: {current_models}")
        print(f"models:{current_models}")
        model_status = {}
        
        for model in BEDROCK_MODELS:
            print(model)
            # Bedrock models are always available
            model_status[model] = {
                "status": "available",
                "message": f"{model} æ¨¡å‹å·²å‡†å¤‡å¥½ï¼Œéšæ—¶å¯ä»¥ä½¿ç”¨",
                "type": "bedrock"
            }
        for model in EMD_MODELS:
            # EMD models need deployment check
            # current_models = {
            #     "inprogress": inprogress,
            #     "deployed": deployed,
            # }
            if model in current_models["deployed"]:
                model_status[model] = {
                    "status": "deployed",
                    "message": f"{model} æ¨¡å‹å·²éƒ¨ç½²ï¼Œå¯ä»¥ä½¿ç”¨",
                    "tag": current_models["deployed"][model]['tag'],
                    "type": "emd"
                }
            elif model in current_models["inprogress"]:
                model_status[model] = {
                    "status": "inprogress",
                    "message": f"{model} æ¨¡å‹éƒ¨ç½²ä¸­ï¼Œè¯·ç­‰å¾…",
                    "tag": current_models["inprogress"][model]['tag'],
                    "type": "emd"
                }
            elif model in current_models["failed"]:
                model_status[model] = {
                    "status": "failed",
                    "message": f"{model} æ¨¡å‹éƒ¨ç½²å¤±è´¥",
                    "tag": current_models["failed"][model]['tag'],
                    "type": "emd"
                }
            else:
                model_status[model] = {
                    "status": "not_deployed",
                    "message": f"{model} æ¨¡å‹éœ€è¦éƒ¨ç½²ï¼Œæˆ–éƒ¨ç½²å¤±è´¥",
                    "type": "emd"
                }
            # else:
            #     model_status[model] = {
            #         "status": "unknown",
            #         "message": f"æœªçŸ¥æ¨¡å‹: {model}",
            #         "type": "unknown"
            #     }
        print("model_status", model_status)
        return jsonify({
            "status": "success",
            "model_status": model_status,
            "deployed_models": current_models["deployed"]
        })
        
    except Exception as e:
        logging.error(f"Check model status error: {e}")
        return jsonify({"error": str(e)}), 500

# å‹åŠ›æµ‹è¯•ç›¸å…³åŠŸèƒ½
stress_test_sessions = {}

def run_stress_test_evalscope(model_key, test_params, session_id):
    """ä½¿ç”¨evalscope Python APIè¿è¡Œå‹åŠ›æµ‹è¯•"""
    global stress_test_sessions
    
    try:
        logging.info(f"[STRESS_TEST] Starting stress test for session {session_id} with model {model_key}")
        logging.info(f"[STRESS_TEST] Test params: {test_params}")
        
        # åˆå§‹åŒ–æµ‹è¯•çŠ¶æ€
        stress_test_sessions[session_id] = {
            "status": "preparing",
            "model": model_key,
            "start_time": datetime.now().isoformat(),
            "progress": 0,
            "message": "å‡†å¤‡æµ‹è¯•ç¯å¢ƒ...",
            "results": None,
            "error": None
        }
        logging.info(f"[STRESS_TEST] Session {session_id} initialized")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        logging.info(f"[STRESS_TEST] Checking if model {model_key} is available")
        deployed_models = get_current_models()["deployed"]
        logging.info(f"[STRESS_TEST] Deployed models: {list(deployed_models.keys())}")
        logging.info(f"[STRESS_TEST] Bedrock models: {list(BEDROCK_MODELS.keys())}")
        
        if model_key not in deployed_models and model_key not in BEDROCK_MODELS:
            error_msg = f"æ¨¡å‹ {model_key} æœªéƒ¨ç½²æˆ–ä¸å¯ç”¨"
            logging.error(f"[STRESS_TEST] {error_msg}")
            raise Exception(error_msg)
        
        logging.info(f"[STRESS_TEST] Model {model_key} is available. Setting status to running...")
        stress_test_sessions[session_id]["status"] = "running"
        stress_test_sessions[session_id]["message"] = "æ­£åœ¨è¿è¡Œå‹åŠ›æµ‹è¯•..."
        stress_test_sessions[session_id]["progress"] = 10
        logging.info(f"[STRESS_TEST] Session {session_id} status updated to running")
        
        # è·å–æµ‹è¯•å‚æ•°
        num_requests = test_params.get('num_requests', 100)
        concurrency = test_params.get('concurrency', 10)
        input_tokens_range = test_params.get('input_tokens_range', [50, 100])
        output_tokens_range = test_params.get('output_tokens_range', [100, 200])
        temperature = test_params.get('temperature', 0.1)
        
        logging.info(f"[STRESS_TEST] Test parameters - requests: {num_requests}, concurrency: {concurrency}")
        logging.info(f"[STRESS_TEST] Token ranges - input: {input_tokens_range}, output: {output_tokens_range}")
        
        # å‡†å¤‡evalscope Python API
        stress_test_sessions[session_id]["progress"] = 30
        stress_test_sessions[session_id]["message"] = "é…ç½®evalscopeå‚æ•°..."
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºAPIé…ç½®
        if model_key in EMD_MODELS:
            # EMDæ¨¡å‹ä½¿ç”¨OpenAIæ ¼å¼API
            model_id = EMD_MODELS[model_key]["model_path"]
            deployed_tag = deployed_models[model_key]['tag']
            base_url = get_emd_base_url(model_id, deployed_tag)
            api_url = f"{base_url}/v1/chat/completions"
            model_endpoint = f"{model_id}/{deployed_tag}"
        else:
            # Bedrockæ¨¡å‹æš‚ä¸æ”¯æŒï¼ˆéœ€è¦æœ¬åœ°APIè°ƒç”¨ï¼‰
            raise Exception(f"Bedrockæ¨¡å‹ {model_key} æš‚ä¸æ”¯æŒç›´æ¥å‹åŠ›æµ‹è¯•ï¼Œè¯·ä½¿ç”¨EMDæ¨¡å‹")
        
        # ä½¿ç”¨evalscope Python API
        try:
            # åŠ¨æ€å¯¼å…¥evalscopeæ¨¡å—ï¼ˆåœ¨evalscopeç¯å¢ƒä¸­è¿è¡Œï¼‰
            import sys
            evalscope_env_path = "/home/ec2-user/SageMaker/efs/conda_envs/evalscope/lib/python3.10/site-packages"
            if evalscope_env_path not in sys.path:
                sys.path.insert(0, evalscope_env_path)
            
            # åˆ›å»ºä¸´æ—¶Pythonè„šæœ¬æ–‡ä»¶ - åŒ…å«ä¿®å¤EMD APIå…¼å®¹æ€§çš„è¡¥ä¸
            evalscope_script_content = f"""
import sys
sys.path.insert(0, '/home/ec2-user/SageMaker/efs/conda_envs/evalscope/lib/python3.10/site-packages')

# ä¿®å¤EMD APIæµå¼å“åº”å…¼å®¹æ€§é—®é¢˜
def patch_openai_api():
    from evalscope.perf.plugin.api.openai_api import OpenaiPlugin
    
    # ä¿å­˜åŸå§‹æ–¹æ³•
    original_calculate_tokens = OpenaiPlugin._OpenaiPlugin__calculate_tokens_from_content
    
    def patched_calculate_tokens(self, request, delta_contents):
        try:
            # æœ€å¼ºåŠ›çš„æ•°æ®æ¸…æ´—å’Œç±»å‹è½¬æ¢
            normalized_contents = []
            
            # æ£€æŸ¥delta_contentsæœ¬èº«çš„ç±»å‹
            if delta_contents is None:
                return 0, 0
            
            # ç¡®ä¿delta_contentsæ˜¯å¯è¿­ä»£çš„
            if not hasattr(delta_contents, '__iter__') or isinstance(delta_contents, (str, bytes)):
                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•åŒ…è£…æˆåˆ—è¡¨
                delta_contents = [delta_contents] if delta_contents is not None else []
            
            for i, choice_contents in enumerate(delta_contents):
                try:
                    if choice_contents is None:
                        normalized_contents.append([])
                    elif isinstance(choice_contents, (list, tuple)):
                        # å¤„ç†åˆ—è¡¨/å…ƒç»„ï¼šé€’å½’æ¸…ç†æ‰€æœ‰å…ƒç´ 
                        clean_list = []
                        for item in choice_contents:
                            if item is not None:
                                if isinstance(item, (str, int, float, bool)):
                                    clean_list.append(str(item))
                                else:
                                    clean_list.append(str(item) if hasattr(item, '__str__') else '')
                        normalized_contents.append(clean_list)
                    elif isinstance(choice_contents, (str, int, float, bool)):
                        # åŸºç¡€ç±»å‹ï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
                        normalized_contents.append([str(choice_contents)])
                    elif isinstance(choice_contents, dict):
                        # å­—å…¸ç±»å‹ï¼šæå–æ–‡æœ¬å†…å®¹
                        text_content = choice_contents.get('text', choice_contents.get('content', str(choice_contents)))
                        normalized_contents.append([str(text_content)])
                    else:
                        # å…¶ä»–ç±»å‹ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        try:
                            str_content = str(choice_contents) if choice_contents is not None else ''
                            normalized_contents.append([str_content])
                        except:
                            normalized_contents.append([''])
                except Exception as item_error:
                    print(f"Error processing item {{i}}: {{item_error}}, type: {{type(choice_contents)}}")
                    normalized_contents.append([''])
            
            # ç¡®ä¿æˆ‘ä»¬æœ‰æœ‰æ•ˆçš„æ•°æ®ç»“æ„
            if not normalized_contents:
                return 0, 0
            
            return original_calculate_tokens(self, request, normalized_contents)
            
        except Exception as e:
            print(f"Primary patch failed: {{e}}, delta_contents type: {{type(delta_contents)}}")
            # è¶…çº§å®‰å…¨çš„å›é€€ç­–ç•¥
            try:
                # å°è¯•å®Œå…¨é‡æ„æ•°æ®
                if delta_contents is None:
                    return 0, 0
                
                # å¼ºåˆ¶åˆ›å»ºå®‰å…¨çš„å­—ç¬¦ä¸²åˆ—è¡¨ç»“æ„
                safe_structure = []
                
                if isinstance(delta_contents, (list, tuple)):
                    for item in delta_contents:
                        if item is None:
                            safe_structure.append([])
                        else:
                            # æ— è®ºä»€ä¹ˆç±»å‹ï¼Œéƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
                            safe_structure.append([str(item)[:1000]])  # é™åˆ¶é•¿åº¦é˜²æ­¢å†…å­˜é—®é¢˜
                else:
                    # å•ä¸ªé¡¹ç›®ï¼ŒåŒ…è£…æˆåˆ—è¡¨ç»“æ„
                    safe_structure.append([str(delta_contents)[:1000]] if delta_contents is not None else [''])
                
                return original_calculate_tokens(self, request, safe_structure)
                
            except Exception as fallback_error:
                print(f"All fallback attempts failed: {{fallback_error}}")
                # æœ€ç»ˆå®‰å…¨ç½‘ï¼šè¿”å›ä¿å®ˆçš„tokenä¼°è®¡
                try:
                    # å°è¯•åŸºäºè¯·æ±‚å†…å®¹ä¼°ç®—tokenæ•°
                    if hasattr(request, 'get'):
                        content = request.get('messages', [])
                        if content:
                            text_content = str(content)
                            estimated_tokens = max(len(text_content) // 4, 10)  # ç²—ç•¥ä¼°è®¡
                            return estimated_tokens, estimated_tokens
                except:
                    pass
                
                return 10, 10  # æœ€ä¿å®ˆçš„é»˜è®¤å€¼
    
    # åº”ç”¨è¡¥ä¸
    OpenaiPlugin._OpenaiPlugin__calculate_tokens_from_content = patched_calculate_tokens

# åº”ç”¨è¡¥ä¸
patch_openai_api()

from evalscope.perf.main import run_perf_benchmark
from evalscope.perf.arguments import Arguments
import json

# åˆ›å»ºæµ‹è¯•é…ç½® - ä¿æŒæµå¼ä¼ è¾“ä»¥è·å¾—TTFTå’Œå»¶è¿ŸæŒ‡æ ‡
task_cfg = Arguments(
    parallel=[{concurrency}],
    number=[{num_requests}],
    model='{model_endpoint}',
    url='{api_url}',
    api='openai',
    dataset='random',
    max_tokens={max(output_tokens_range)},
    min_tokens={min(output_tokens_range)},
    min_prompt_length={min(input_tokens_range)},
    max_prompt_length={max(input_tokens_range)},
    tokenizer_path='/home/ec2-user/SageMaker/efs/Models/Qwen3-32B-AWQ',
    temperature={temperature},
    stream=True  # ä¿æŒæµå¼ä¼ è¾“ä»¥è·å¾—å‡†ç¡®çš„TTFTå’Œå»¶è¿ŸæŒ‡æ ‡
)

# è¿è¡ŒåŸºå‡†æµ‹è¯•
try:
    results = run_perf_benchmark(task_cfg)
    print("EVALSCOPE_SUCCESS:", json.dumps(results) if results else "{{}}")
except Exception as e:
    print("EVALSCOPE_ERROR:", str(e))
    import traceback
    traceback.print_exc()
"""
            
            # åˆ›å»ºä¸´æ—¶è„šæœ¬æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as script_file:
                script_file.write(evalscope_script_content)
                script_path = script_file.name
            
            stress_test_sessions[session_id]["progress"] = 20
            stress_test_sessions[session_id]["message"] = "å‡†å¤‡æ‰§è¡ŒevalscopeåŸºå‡†æµ‹è¯•..."
            
            # åœ¨evalscopeç¯å¢ƒä¸­æ‰§è¡Œè„šæœ¬
            env = os.environ.copy()
            evalscope_cmd = f"source /opt/conda/etc/profile.d/conda.sh && conda activate evalscope && python {script_path}"
            
            logging.info(f"[STRESS_TEST] Executing evalscope Python API...")
            stress_test_sessions[session_id]["progress"] = 30
            stress_test_sessions[session_id]["message"] = "æ‰§è¡ŒevalscopeåŸºå‡†æµ‹è¯•..."
            
            try:
                result = subprocess.run(
                    evalscope_cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶
                    env=env,
                    executable='/bin/bash'
                )
                
                logging.info(f"[STRESS_TEST] Evalscope Python API completed with return code: {result.returncode}")
                if result.stdout:
                    logging.info(f"[STRESS_TEST] Evalscope stdout: {result.stdout}")
                if result.stderr:
                    logging.error(f"[STRESS_TEST] Evalscope stderr: {result.stderr}")
                
                stress_test_sessions[session_id]["progress"] = 90
                stress_test_sessions[session_id]["message"] = "æ­£åœ¨å¤„ç†æµ‹è¯•ç»“æœ..."
                
            finally:
                # æ¸…ç†ä¸´æ—¶è„šæœ¬æ–‡ä»¶
                try:
                    os.unlink(script_path)
                except:
                    pass
            
            # è§£æç»“æœ
            if "EVALSCOPE_SUCCESS:" in result.stdout:
                # æå–æˆåŠŸç»“æœ
                success_line = [line for line in result.stdout.split('\n') if 'EVALSCOPE_SUCCESS:' in line][0]
                results_str = success_line.replace('EVALSCOPE_SUCCESS:', '').strip()
                
                try:
                    # å°è¯•è§£æJSONç»“æœ
                    import json
                    raw_results = json.loads(results_str) if results_str else []
                    
                    # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                    if isinstance(raw_results, list) and len(raw_results) >= 2:
                        summary_data = raw_results[0]
                        percentile_data = raw_results[1] if len(raw_results) > 1 else {}
                        
                        # åˆ›å»ºå‰ç«¯å…¼å®¹çš„ç»“æœæ ¼å¼
                        results = {
                            "qps": summary_data.get("Request throughput (req/s)", 0),
                            "avg_ttft": summary_data.get("Average time to first token (s)", 0),
                            "avg_latency": summary_data.get("Average latency (s)", 0),
                            "tokens_per_second": summary_data.get("Total token throughput (tok/s)", 0),
                            "p50_ttft": percentile_data.get("TTFT (s)", [0] * 10)[4] if "TTFT (s)" in percentile_data else 0,
                            "p99_ttft": percentile_data.get("TTFT (s)", [0] * 10)[9] if "TTFT (s)" in percentile_data else 0,
                            "p50_latency": percentile_data.get("Latency (s)", [0] * 10)[4] if "Latency (s)" in percentile_data else 0,
                            "p99_latency": percentile_data.get("Latency (s)", [0] * 10)[9] if "Latency (s)" in percentile_data else 0,
                            "summary": summary_data,
                            "percentiles": percentile_data,
                            "detailed_metrics": {
                                "ttft_distribution": percentile_data.get("TTFT (s)", []),
                                "latency_distribution": percentile_data.get("Latency (s)", []),
                                "input_tokens": percentile_data.get("Input tokens", []),
                                "output_tokens": percentile_data.get("Output tokens", [])
                            }
                        }
                    else:
                        # å¤‡ç”¨è§£ææ–¹æ¡ˆ
                        results = {
                            "summary": raw_results[0] if raw_results else {},
                            "raw_data": raw_results
                        }
                        
                except Exception as parse_error:
                    logging.error(f"[STRESS_TEST] JSON parsing failed: {parse_error}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ
                    results = parse_evalscope_output(result.stdout)
                
                stress_test_sessions[session_id].update({
                    "status": "completed",
                    "progress": 100,
                    "message": "æµ‹è¯•å®Œæˆ",
                    "results": results,
                    "end_time": datetime.now().isoformat(),
                    "raw_output": result.stdout
                })
                
            elif "EVALSCOPE_ERROR:" in result.stdout:
                # æå–é”™è¯¯ä¿¡æ¯
                error_line = [line for line in result.stdout.split('\n') if 'EVALSCOPE_ERROR:' in line][0]
                error_msg = error_line.replace('EVALSCOPE_ERROR:', '').strip()
                
                stress_test_sessions[session_id].update({
                    "status": "failed",
                    "progress": 100,
                    "message": "æµ‹è¯•å¤±è´¥",
                    "error": error_msg,
                    "raw_output": result.stdout,
                    "end_time": datetime.now().isoformat()
                })
            else:
                # æœªæ‰¾åˆ°æ˜ç¡®çš„æˆåŠŸæˆ–é”™è¯¯æ ‡è®°
                if result.returncode == 0:
                    # è¿›ç¨‹æˆåŠŸä½†è¾“å‡ºæ ¼å¼å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ
                    results = parse_evalscope_output(result.stdout)
                    stress_test_sessions[session_id].update({
                        "status": "completed",
                        "progress": 100,
                        "message": "æµ‹è¯•å®Œæˆ",
                        "results": results,
                        "end_time": datetime.now().isoformat(),
                        "raw_output": result.stdout
                    })
                else:
                    stress_test_sessions[session_id].update({
                        "status": "failed",
                        "progress": 100,
                        "message": "æµ‹è¯•å¤±è´¥",
                        "error": result.stderr or "æœªçŸ¥é”™è¯¯",
                        "raw_output": result.stdout,
                        "end_time": datetime.now().isoformat()
                    })
                    
        except Exception as api_error:
            logging.error(f"[STRESS_TEST] Evalscope API error: {str(api_error)}")
            raise Exception(f"Evalscope APIè°ƒç”¨å¤±è´¥: {str(api_error)}")
                
    except Exception as e:
        logging.error(f"[STRESS_TEST] Exception in stress test session {session_id}: {str(e)}")
        logging.error(f"[STRESS_TEST] Exception traceback: {traceback.format_exc()}")
        stress_test_sessions[session_id].update({
            "status": "failed",
            "progress": 100,
            "message": f"æµ‹è¯•å¼‚å¸¸: {str(e)}",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })

def parse_evalscope_output(output):
    """è§£æevalscopeè¾“å‡ºç»“æœ"""
    try:
        # ç®€å•çš„è¾“å‡ºè§£æï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®evalscopeçš„å…·ä½“è¾“å‡ºæ ¼å¼è°ƒæ•´
        lines = output.split('\n')
        results = {
            "summary": {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "average_latency": 0,
                "min_latency": 0,
                "max_latency": 0,
                "p95_latency": 0,
                "p99_latency": 0,
                "requests_per_second": 0,
                "average_ttft": 0,
                "tokens_per_second": 0
            },
            "detailed_metrics": [],
            "errors": []
        }
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„è§£æé€»è¾‘
        for line in lines:
            if "Total requests" in line:
                try:
                    results["summary"]["total_requests"] = int(line.split(":")[-1].strip())
                except:
                    pass
            elif "Successful" in line:
                try:
                    results["summary"]["successful_requests"] = int(line.split(":")[-1].strip())
                except:
                    pass
            elif "Average latency" in line:
                try:
                    results["summary"]["average_latency"] = float(line.split(":")[-1].strip().replace('ms', ''))
                except:
                    pass
        
        return results
    except Exception as e:
        return {"error": f"Failed to parse results: {str(e)}", "raw_output": output}

@app.route('/api/stress-test/start', methods=['POST'])
def start_stress_test():
    """å¯åŠ¨å‹åŠ›æµ‹è¯•"""
    try:
        data = request.json
        model_key = data.get('model')
        test_params = data.get('params', {})
        
        if not model_key:
            return jsonify({"error": "Model is required"}), 400
        
        # ç”Ÿæˆå”¯ä¸€çš„æµ‹è¯•ä¼šè¯ID
        import uuid
        session_id = str(uuid.uuid4())[:8]
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæµ‹è¯•
        thread = threading.Thread(
            target=run_stress_test_evalscope,
            args=(model_key, test_params, session_id),
            daemon=True
        )
        thread.start()
        
        return jsonify({
            "status": "success",
            "session_id": session_id,
            "message": "å‹åŠ›æµ‹è¯•å·²å¯åŠ¨"
        })
        
    except Exception as e:
        logging.error(f"Start stress test error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/stress-test/status/<session_id>', methods=['GET'])
def get_stress_test_status(session_id):
    """è·å–å‹åŠ›æµ‹è¯•çŠ¶æ€"""
    try:
        if session_id not in stress_test_sessions:
            logging.warning(f"[STRESS_TEST_STATUS] Session {session_id} not found in sessions")
            return jsonify({"error": "Test session not found"}), 404
        
        session_data = stress_test_sessions[session_id]
        logging.info(f"[STRESS_TEST_STATUS] Session {session_id} status: {session_data.get('status')}, progress: {session_data.get('progress')}")
        
        return jsonify({
            "status": "success",
            "test_session": session_data
        })
        
    except Exception as e:
        logging.error(f"Get stress test status error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/stress-test/download/<session_id>', methods=['GET'])
def download_stress_test_report(session_id):
    """ä¸‹è½½å‹åŠ›æµ‹è¯•PDFæŠ¥å‘Š"""
    try:
        if session_id not in stress_test_sessions:
            return jsonify({"error": "Test session not found"}), 404
        
        session_data = stress_test_sessions[session_id]
        
        if session_data.get("status") != "completed" or not session_data.get("results"):
            return jsonify({"error": "Test not completed or no results available"}), 400
        
        # ç”ŸæˆPDFæŠ¥å‘Š
        pdf_content = generate_pdf_report(session_data, session_id)
        
        # è¿”å›PDFæ–‡ä»¶
        response = make_response(pdf_content)
        response.headers['Content-Type'] = 'application/pdf'
        response.headers['Content-Disposition'] = f'attachment; filename=stress_test_report_{session_id}.pdf'
        return response
        
    except Exception as e:
        logging.error(f"Download stress test report error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

def generate_pdf_report(session_data, session_id):
    """ç”ŸæˆPDFæ ¼å¼çš„å‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from datetime import datetime
    import io
    import os
    
    # æ³¨å†Œä¸­æ–‡å­—ä½“
    try:
        # å°è¯•æ³¨å†Œä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨ç³»ç»Ÿå­—ä½“è·¯å¾„
        chinese_font_paths = [
            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',  # ä¼˜å…ˆä½¿ç”¨Noto CJK
            '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',  # WenQuanYiå¾®ç±³é»‘
            '/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc',  # WenQuanYiæ­£é»‘
            '/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc',  # Linux Noto
            '/System/Library/Fonts/PingFang.ttc',  # macOS
        ]
        
        font_registered = False
        for font_path in chinese_font_paths:
            if os.path.exists(font_path):
                try:
                    pdfmetrics.registerFont(TTFont('ChineseFont', font_path))
                    font_registered = True
                    break
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨Helveticaä½œä¸ºå›é€€
        if not font_registered:
            chinese_font_name = 'Helvetica'
        else:
            chinese_font_name = 'ChineseFont'
            
    except Exception as e:
        print(f"Font registration error: {e}")
        chinese_font_name = 'Helvetica'
    
    # åˆ›å»ºPDFç¼“å†²åŒº
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
    
    # è·å–æ ·å¼
    styles = getSampleStyleSheet()
    
    # è‡ªå®šä¹‰æ ·å¼ï¼ˆä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼‰
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontName=chinese_font_name,
        fontSize=24,
        spaceAfter=30,
        alignment=TA_CENTER,
        textColor=colors.darkblue
    )
    
    heading_style = ParagraphStyle(
        'CustomHeading',
        parent=styles['Heading2'],
        fontName=chinese_font_name,
        fontSize=16,
        spaceAfter=15,
        spaceBefore=20,
        textColor=colors.darkblue
    )
    
    # æ„å»ºPDFå†…å®¹
    story = []
    
    # æ ‡é¢˜
    story.append(Paragraph("å‹åŠ›æµ‹è¯•æŠ¥å‘Š", title_style))
    story.append(Spacer(1, 20))
    
    # æµ‹è¯•ä¿¡æ¯
    story.append(Paragraph("æµ‹è¯•åŸºæœ¬ä¿¡æ¯", heading_style))
    
    test_info_data = [
        ['é¡¹ç›®', 'å€¼'],
        ['æ¨¡å‹', session_data.get('model', 'N/A')],
        ['ä¼šè¯ID', session_id],
        ['å¼€å§‹æ—¶é—´', session_data.get('start_time', 'N/A')],
        ['ç»“æŸæ—¶é—´', session_data.get('end_time', 'N/A')],
        ['çŠ¶æ€', 'å®Œæˆ' if session_data.get('status') == 'completed' else session_data.get('status', 'N/A')]
    ]
    
    test_info_table = Table(test_info_data, colWidths=[2*inch, 4*inch])
    test_info_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 0), (-1, 0), chinese_font_name),
        ('FONTSIZE', (0, 0), (-1, 0), 14),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
        ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
        ('FONTNAME', (0, 1), (-1, -1), chinese_font_name),
        ('GRID', (0, 0), (-1, -1), 1, colors.black)
    ]))
    
    story.append(test_info_table)
    story.append(Spacer(1, 20))
    
    # æ€§èƒ½æŒ‡æ ‡æ¦‚è§ˆ
    results = session_data.get('results', {})
    if results:
        story.append(Paragraph("æ€§èƒ½æŒ‡æ ‡æ¦‚è§ˆ", heading_style))
        
        metrics_data = [
            ['æŒ‡æ ‡', 'æ•°å€¼', 'å•ä½'],
            ['QPS (æ¯ç§’æŸ¥è¯¢æ•°)', f"{results.get('qps', 0):.2f}", 'queries/sec'],
            ['å¹³å‡é¦–å­—å»¶è¿Ÿ', f"{results.get('avg_ttft', 0):.3f}", 'seconds'],
            ['å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿ', f"{results.get('avg_latency', 0):.3f}", 'seconds'],
            ['ååé‡', f"{results.get('tokens_per_second', 0):.2f}", 'tokens/sec'],
            ['P50 é¦–å­—å»¶è¿Ÿ', f"{results.get('p50_ttft', 0):.3f}", 'seconds'],
            ['P99 é¦–å­—å»¶è¿Ÿ', f"{results.get('p99_ttft', 0):.3f}", 'seconds'],
            ['P50 ç«¯åˆ°ç«¯å»¶è¿Ÿ', f"{results.get('p50_latency', 0):.3f}", 'seconds'],
            ['P99 ç«¯åˆ°ç«¯å»¶è¿Ÿ', f"{results.get('p99_latency', 0):.3f}", 'seconds']
        ]
        
        metrics_table = Table(metrics_data, colWidths=[2.5*inch, 1.5*inch, 2*inch])
        metrics_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), chinese_font_name),
            ('FONTSIZE', (0, 0), (-1, 0), 12),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.lightblue),
            ('FONTNAME', (0, 1), (-1, -1), chinese_font_name),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
            ('FONTSIZE', (0, 1), (-1, -1), 10)
        ]))
        
        story.append(metrics_table)
        story.append(Spacer(1, 20))
        
        # ç™¾åˆ†ä½æ•°æ®è¯¦ç»†è¡¨
        percentiles = results.get('percentiles', {})
        if percentiles and 'Percentiles' in percentiles:
            story.append(PageBreak())
            story.append(Paragraph("ç™¾åˆ†ä½æ•°æ®è¯¦ç»†è¡¨", heading_style))
            
            percentile_labels = percentiles['Percentiles']
            percentile_data = [['ç™¾åˆ†ä½', 'TTFT (s)', 'å»¶è¿Ÿ (s)', 'ITL (s)', 'TPOT (s)', 'è¾“å…¥Token', 'è¾“å‡ºToken', 'ååé‡ (tok/s)']]
            
            for i, label in enumerate(percentile_labels):
                row = [
                    label,
                    f"{percentiles.get('TTFT (s)', [0]*len(percentile_labels))[i]:.4f}",
                    f"{percentiles.get('Latency (s)', [0]*len(percentile_labels))[i]:.4f}",
                    f"{percentiles.get('ITL (s)', [0]*len(percentile_labels))[i]:.4f}",
                    f"{percentiles.get('TPOT (s)', [0]*len(percentile_labels))[i]:.4f}",
                    str(percentiles.get('Input tokens', [0]*len(percentile_labels))[i]),
                    str(percentiles.get('Output tokens', [0]*len(percentile_labels))[i]),
                    f"{percentiles.get('Output (tok/s)', [0]*len(percentile_labels))[i]:.2f}"
                ]
                percentile_data.append(row)
            
            percentile_table = Table(percentile_data, colWidths=[0.8*inch]*8)
            percentile_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkgreen),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), chinese_font_name),
                ('FONTSIZE', (0, 0), (-1, 0), 9),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgreen),
                ('FONTNAME', (0, 1), (-1, -1), chinese_font_name),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('FONTSIZE', (0, 1), (-1, -1), 8)
            ]))
            
            story.append(percentile_table)
            story.append(Spacer(1, 20))
        
        # è¯¦ç»†æ‘˜è¦ä¿¡æ¯
        summary = results.get('summary', {})
        if summary:
            story.append(Paragraph("æµ‹è¯•æ‰§è¡Œæ‘˜è¦", heading_style))
            
            summary_data = [
                ['é¡¹ç›®', 'å€¼'],
                ['æµ‹è¯•è€—æ—¶', f"{summary.get('Time taken for tests (s)', 0):.2f} ç§’"],
                ['å¹¶å‘æ•°', str(summary.get('Number of concurrency', 0))],
                ['æ€»è¯·æ±‚æ•°', str(summary.get('Total requests', 0))],
                ['æˆåŠŸè¯·æ±‚æ•°', str(summary.get('Succeed requests', 0))],
                ['å¤±è´¥è¯·æ±‚æ•°', str(summary.get('Failed requests', 0))],
                ['å¹³å‡è¾“å…¥Tokenæ•°', f"{summary.get('Average input tokens per request', 0):.1f}"],
                ['å¹³å‡è¾“å‡ºTokenæ•°', f"{summary.get('Average output tokens per request', 0):.1f}"],
                ['å¹³å‡Tokené—´å»¶è¿Ÿ', f"{summary.get('Average inter-token latency (s)', 0):.4f} ç§’"],
                ['å¹³å‡æ¯è¾“å‡ºTokenæ—¶é—´', f"{summary.get('Average time per output token (s)', 0):.4f} ç§’"]
            ]
            
            summary_table = Table(summary_data, colWidths=[3*inch, 3*inch])
            summary_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.purple),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), chinese_font_name),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lavender),
                ('FONTNAME', (0, 1), (-1, -1), chinese_font_name),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('FONTSIZE', (0, 1), (-1, -1), 10)
            ]))
            
            story.append(summary_table)
    
    # é¡µè„šä¿¡æ¯
    footer_style = ParagraphStyle('Footer', fontName=chinese_font_name, fontSize=10, alignment=TA_CENTER, textColor=colors.grey)
    story.append(Spacer(1, 30))
    story.append(Paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", footer_style))
    story.append(Paragraph("ç”± EMD æ¨ç†å¹³å°ç”Ÿæˆ", footer_style))
    
    # æ„å»ºPDF
    doc.build(story)
    
    # è·å–PDFæ•°æ®
    pdf_data = buffer.getvalue()
    buffer.close()
    
    return pdf_data

# @app.route('/api/deploy-selected-models', methods=['POST'])
# def deploy_selected_models():
#     """Deploy selected EMD models (called when user clicks ä¸‹ä¸€æ­¥,ä¸Šä¼ ææ–™)"""
#     data = request.json
#     selected_models = data.get('models', [])
#     # Keep model keys as they are (simplified names like 'qwen2-vl-7b')
#     emd_models = [x for x in selected_models if x in EMD_MODELS]
    
#     if not emd_models:
#         return jsonify({"error": "No EMD models specified"}), 400
    
#     try:
#         # Filter to only EMD models that need deployment
#         deployed_models = get_current_models()
#         models_to_deploy = []
        
#         for model in emd_models:
#             if model not in deployed_models:
#                 models_to_deploy.append(model)
        
#         if not models_to_deploy:
#             return jsonify({
#                 "status": "success",
#                 "message": "æ‰€æœ‰æ‰€é€‰æ¨¡å‹éƒ½å·²éƒ¨ç½²æˆ–ä¸éœ€è¦éƒ¨ç½²",
#                 "models_to_deploy": []
#             })
        
#         # Start deployment for models that need it
#         deployment_info = {}
#         for model_name in models_to_deploy:
#             try:
#                 tag = deploy_model_if_not_exist(model_name)
#                 deployment_info[model_name] = {
#                     "status": "deploying",
#                     "tag": tag,
#                     "message": f"å¼€å§‹éƒ¨ç½² {model_name}ï¼Œè¯·è€å¿ƒç­‰å¾…..."
#                 }
#             except Exception as e:
#                 deployment_info[model_name] = {
#                     "status": "error",
#                     "message": f"éƒ¨ç½² {model_name} å¤±è´¥: {str(e)}"
#                 }
        
#         return jsonify({
#             "status": "success",
#             "deployment_info": deployment_info,
#             "models_deployed": len(deployment_info),
#             "message": f"å¼€å§‹éƒ¨ç½² {len(deployment_info)} ä¸ªæ¨¡å‹"
#         })
        
#     except Exception as e:
#         logging.error(f"Deploy selected models error: {e}")
#         return jsonify({"error": str(e)}), 500

# @app.route('/api/emd/deployment-stream', methods=['GET'])
# def stream_emd_deployment_status():
#     """Stream real-time deployment status updates"""
#     def generate():
#         """Generator for streaming deployment updates"""
#         models_to_watch = request.args.get('models', '').split(',') if request.args.get('models') else []
        
#         # Send initial status
#         yield f"data: {json.dumps({'type': 'initial', 'deployments': deployment_status, 'bootstrap_status': bootstrap_status})}\n\n"
        
#         last_status = {}
#         check_interval = 2  # Check every 2 seconds
#         max_duration = 1800  # Maximum 30 minutes
#         start_time = time.time()
        
#         while time.time() - start_time < max_duration:
#             try:
#                 # Check if any deployments are still active
#                 any_active = any(
#                     status.get("status") in ["checking_bootstrap", "bootstrapping", "deploying"]
#                     for status in deployment_status.values()
#                 )
                
#                 # If watching specific models, check if they're still active
#                 if models_to_watch:
#                     models_active = any(
#                         model in deployment_status and 
#                         deployment_status[model].get("status") in ["checking_bootstrap", "bootstrapping", "deploying"]
#                         for model in models_to_watch
#                     )
#                     if not models_active and models_to_watch:
#                         # Send final status and close
#                         yield f"data: {json.dumps({'type': 'complete', 'deployments': deployment_status})}\n\n"
#                         break
#                 elif not any_active:
#                     # No active deployments, send final status
#                     yield f"data: {json.dumps({'type': 'complete', 'deployments': deployment_status})}\n\n"
#                     break
                
#                 # Check for status changes
#                 current_status = {k: v for k, v in deployment_status.items()}
#                 if current_status != last_status:
#                     # Send update
#                     update_data = {
#                         'type': 'update',
#                         'deployments': current_status,
#                         'bootstrap_status': bootstrap_status,
#                         'deployed_models': get_current_models(),
#                         'timestamp': datetime.now().isoformat()
#                     }
#                     yield f"data: {json.dumps(update_data)}\n\n"
#                     last_status = current_status.copy()
                
#                 # Send heartbeat
#                 yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\n\n"
                
#                 time.sleep(check_interval)
                
#             except Exception as e:
#                 yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
#                 break
        
#         # Send final completion message
#         yield f"data: {json.dumps({'type': 'stream_ended'})}\n\n"
    
#     return Response(
#         generate(),
#         mimetype='text/plain',
#         headers={
#             'Cache-Control': 'no-cache',
#             'Connection': 'keep-alive',
#             'Content-Type': 'text/event-stream',
#             'Access-Control-Allow-Origin': '*'
#         }
#     )

def test_emd_integration():
    """Test EMD integration - useful for debugging"""
    print("ğŸ§ª Testing EMD Integration...")
    
    # Test SageMaker client
    try:
        client = create_emd_sagemaker_client("Qwen2.5-0.5B-Instruct")
        if client:
            print(f"âœ… SageMaker client created: {client.endpoint_name}")
        else:
            print("âŒ SageMaker client creation failed")
    except Exception as e:
        print(f"âŒ SageMaker test error: {e}")
    
    # Test OpenAI client
    try:
        client = create_emd_openai_client()
        if client:
            print(f"âœ… OpenAI client created with base URL: {client.base_url}")
        else:
            print("âŒ OpenAI client creation failed")
    except Exception as e:
        print(f"âŒ OpenAI test error: {e}")

if __name__ == '__main__':
    
    # é€šè¿‡boto3è·å–å½“å‰åŒºåŸŸ
    try:
        session = boto3.session.Session()
        region = session.region_name or 'us-west-2'  # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        logging.info(f"ä½¿ç”¨boto3è·å–åˆ°åŒºåŸŸ: {region}")
    except Exception as e:
        region = 'us-west-2'  # é»˜è®¤åŒºåŸŸ
        logging.warning(f"æ— æ³•é€šè¿‡boto3è·å–åŒºåŸŸï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
    
    # emdåˆå§‹åŒ–
    try:
        init_emd_env(region=region)
        logging.info(f"EMD initialized at startup with region: {region}")
    except Exception as e:
        logging.error(f"Startup EMD initialization failed: {e}")

    print("ğŸš€ Starting EMD-integrated Multimodal Inference Platform")
    print(f"ğŸ“‹ Available EMD models: {list(EMD_MODELS.keys())}")
    
    # Get currently deployed models dynamically
    try:
        deployed_models = get_current_models()
        print(f"ğŸ·ï¸ Currently deployed models: {list(deployed_models.keys())}")
        if deployed_models:
            for model, info in deployed_models.items():
                print(f"   - {model}: tag={info['tag']}, endpoint={info.get('endpoint', 'N/A')}")
        else:
            print("âš ï¸ No models currently deployed")
    except Exception as e:
        logging.warning(f"Could not get deployed models at startup: {e}")
    
    print(f"ğŸ·ï¸ Default EMD tag: {DEFAULT_EMD_TAG}")
    print("ğŸŒ Server running on http://localhost:5000")
    
    # Uncomment to test EMD integration on startup
    # test_emd_integration()
    
    app.run(host='0.0.0.0', port=5000) 