from flask import Flask, request, jsonify, Response
import boto3
import json
import logging
from datetime import datetime
from flask_cors import CORS
import threading
import queue
import os
import subprocess
import tempfile
import base64
from PIL import Image
import io
from openai import OpenAI
from emd.sdk.clients.sagemaker_client import SageMakerClient
from emd.sdk.status import get_model_status

app = Flask(__name__)
CORS(app)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='claude35_api.log',
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

bedrock_client = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-west-2'
)

# EMD client configurations
EMD_MODELS = {
    'qwen2-vl-7b': 'Qwen2-VL-7B-Instruct',
    'qwen2.5-vl-32b': 'Qwen2.5-VL-32B-Instruct',
    'gemma-3-4b': 'gemma-3-4b-it',
    'ui-tars-1.5-7b': 'UI-TARS-1.5-7B'
}

def get_emd_base_url():
    """Get EMD base URL from emd status command"""
    try:
        result = subprocess.run(['emd', 'status'], capture_output=True, text=True)
        if result.returncode == 0:
            # Parse the output to find the base URL
            lines = result.stdout.split('\n')
            for line in lines:
                if 'http://' in line and 'elb.amazonaws.com' in line:
                    return line.strip()
        return None
    except Exception as e:
        logging.error(f"Error getting EMD base URL: {e}")
        return None

def create_emd_openai_client():
    """Create OpenAI client for EMD endpoints"""
    base_url = get_emd_base_url()
    if base_url:
        return OpenAI(api_key="", base_url=f"{base_url}/v1")
    return None

def create_emd_sagemaker_client(model_id, model_tag="dev"):
    """Create SageMaker client for EMD endpoints"""
    try:
        return SageMakerClient(model_id=model_id, model_tag=model_tag)
    except Exception as e:
        logging.error(f"Error creating EMD SageMaker client: {e}")
        return None

def encode_image_for_emd(image_base64):
    """Encode image for EMD inference"""
    return image_base64

def get_account_id():
    """åŠ¨æ€è·å–å½“å‰AWSè´¦æˆ·ID"""
    try:
        sts_client = boto3.client('sts')
        identity = sts_client.get_caller_identity()
        return identity['Account']
    except Exception as e:
        logging.warning(f"æ— æ³•è·å–è´¦æˆ·IDï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return "022499040310"  # é»˜è®¤è´¦æˆ·ID

def build_inference_profile_arn(model_type, account_id=None, region='us-west-2'):
    """æ„å»ºæ¨ç†é…ç½®æ–‡ä»¶ARN"""
    if account_id is None:
        account_id = get_account_id()
    
    model_mappings = {
        'claude4': 'us.anthropic.claude-sonnet-4-20250514-v1:0',
        'nova': 'us.amazon.nova-pro-v1:0'
    }
    
    if model_type in model_mappings:
        return f"arn:aws:bedrock:{region}:{account_id}:inference-profile/{model_mappings[model_type]}"
    else:
        # å¦‚æœä¸éœ€è¦æ¨ç†é…ç½®æ–‡ä»¶ï¼Œè¿”å›ç›´æ¥çš„æ¨¡å‹ID
        return model_type

def extract_frames_from_video(video_base64, num_frames=8):
    """ä»base64è§†é¢‘ä¸­æå–å…³é”®å¸§"""
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # ä¿å­˜base64è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            video_path = os.path.join(temp_dir, "input_video.mp4")
            with open(video_path, "wb") as f:
                f.write(base64.b64decode(video_base64))
            
            # è·å–è§†é¢‘æ—¶é•¿
            try:
                duration_cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 '{video_path}'"
                duration = float(subprocess.check_output(duration_cmd, shell=True).decode().strip())
            except Exception as e:
                logging.warning(f"æ— æ³•è·å–è§†é¢‘æ—¶é•¿ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                duration = 10.0  # é»˜è®¤10ç§’
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            frames_dir = os.path.join(temp_dir, "frames")
            os.makedirs(frames_dir, exist_ok=True)
            
            # è®¡ç®—å¸§é—´éš”
            interval = max(duration / num_frames, 0.5)  # æœ€å°é—´éš”0.5ç§’
            
            frame_base64_list = []
            
            # æå–å…³é”®å¸§
            for i in range(num_frames):
                timestamp = min(i * interval, duration - 0.1)  # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘é•¿åº¦
                frame_path = os.path.join(frames_dir, f"frame_{i+1:02d}.jpg")
                
                # ä½¿ç”¨ffmpegæå–å¸§
                ffmpeg_cmd = f"ffmpeg -ss {timestamp} -i '{video_path}' -frames:v 1 -q:v 2 '{frame_path}' -y -loglevel quiet"
                
                try:
                    subprocess.run(ffmpeg_cmd, shell=True, check=True)
                    
                    # å°†å¸§è½¬æ¢ä¸ºbase64
                    if os.path.exists(frame_path):
                        with Image.open(frame_path) as img:
                            # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥èŠ‚çœtoken
                            img.thumbnail((800, 600), Image.Resampling.LANCZOS)
                            buffer = io.BytesIO()
                            img.save(buffer, format="JPEG", quality=85)
                            frame_base64 = base64.b64encode(buffer.getvalue()).decode()
                            frame_base64_list.append(frame_base64)
                            
                except subprocess.CalledProcessError as e:
                    logging.warning(f"æå–ç¬¬{i+1}å¸§å¤±è´¥: {e}")
                    continue
            
            logging.info(f"æˆåŠŸæå–äº† {len(frame_base64_list)} ä¸ªå…³é”®å¸§")
            return frame_base64_list
            
    except Exception as e:
        logging.error(f"è§†é¢‘åˆ‡å¸§å¤„ç†å¤±è´¥: {e}")
        return []

def enhance_prompt_for_video(original_prompt, num_frames):
    """ä¸ºè§†é¢‘åˆ†æå¢å¼ºæç¤ºè¯"""
    video_context = f"These are {num_frames} keyframes extracted from a video at regular intervals. Please analyze these sequential frames to understand the video content and answer the following question: "
    return video_context + original_prompt

def process_model_async(model_name, endpoint_func, data, result_queue):
    """å¼‚æ­¥å¤„ç†å•ä¸ªæ¨¡å‹çš„æ¨ç†"""
    try:
        result = endpoint_func(data)
        result_queue.put({
            'model': model_name,
            'status': 'success',
            'result': result
        })
    except Exception as e:
        result_queue.put({
            'model': model_name,
            'status': 'error',
            'error': str(e)
        })

@app.route('/api/multi-inference', methods=['POST'])
def multi_inference():
    """åŒæ—¶è°ƒç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒæµå¼è¿”å›ç»“æœ"""
    data = request.json
    models = data.get('models', ['claude4', 'claude35', 'nova'])
    
    def generate():
        result_queue = queue.Queue()
        threads = []
        
        # åˆ›å»ºçº¿ç¨‹å¤„ç†æ¯ä¸ªæ¨¡å‹
        for model in models:
            if model == 'claude4':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_claude4_internal(d), data, result_queue)
                )
            elif model == 'claude35':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_claude35_internal(d), data, result_queue)
                )
            elif model == 'nova':
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_nova_internal(d), data, result_queue)
                )
            elif model in EMD_MODELS:
                # EMD model
                thread = threading.Thread(
                    target=process_model_async,
                    args=(model, lambda d: call_emd_model_internal(d, model), data, result_queue)
                )
            else:
                continue
                
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…ç»“æœå¹¶æµå¼è¿”å›
        completed = 0
        total_models = len(threads)
        
        while completed < total_models:
            try:
                result = result_queue.get(timeout=1)
                completed += 1
                
                # å‘é€ç»“æœ
                yield f"data: {json.dumps(result, ensure_ascii=False)}\n\n"
                
            except queue.Empty:
                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        yield f"data: {json.dumps({'type': 'complete'})}\n\n"
    
    return Response(
        generate(),
        mimetype='text/plain',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*'
        }
    )

def call_claude4_internal(data):
    """Claude4æ¨ç†çš„å†…éƒ¨å‡½æ•°"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        content = [{"type": "text", "text": text}]
        
        # å¤„ç†è§†é¢‘å’Œå›¾ç‰‡
        if media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§
            if len(frames) > 0:
                video_base64 = frames[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"type": "text", "text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": frame_base64
                            }
                        })
                    logging.info(f"æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    return jsonify({"error": "Failed to extract frames from video. Please check the video format."}), 400
            else:
                return jsonify({"error": "No video data received."}), 400
        else:  # å¤„ç†å›¾ç‰‡
            for img_base64 in frames:
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": img_base64
                    }
                })
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        # åŠ¨æ€æ„å»ºClaude 4æ¨ç†é…ç½®æ–‡ä»¶ARN
        model_id = build_inference_profile_arn('claude4')
        logging.info(f"Using Claude 4 model ID: {model_id}")
        
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        log_entry['response'] = response_body
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return response_body
    except Exception as e:
        log_entry['error'] = str(e)
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

@app.route('/api/claude4', methods=['POST'])
def call_claude4():
    try:
        result = call_claude4_internal(request.json)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def call_claude35_internal(data):
    """Claude3.5æ¨ç†çš„å†…éƒ¨å‡½æ•°"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        content = [{"type": "text", "text": text}]
        
        # å¤„ç†è§†é¢‘å’Œå›¾ç‰‡
        if media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§
            if len(frames) > 0:
                video_base64 = frames[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"type": "text", "text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/jpeg",
                                "data": frame_base64
                            }
                        })
                    logging.info(f"æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    return jsonify({"error": "Failed to extract frames from video. Please check the video format."}), 400
            else:
                return jsonify({"error": "No video data received."}), 400
        else:  # å¤„ç†å›¾ç‰‡
            for img_base64 in frames:
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": img_base64
                    }
                })
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ]
        }
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        response = bedrock_client.invoke_model(
            modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        log_entry['response'] = response_body
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return response_body
    except Exception as e:
        log_entry['error'] = str(e)
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

@app.route('/api/claude35', methods=['POST'])
def call_claude35():
    try:
        result = call_claude35_internal(request.json)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/nova', methods=['POST'])
def call_nova():
    data = request.json
    text = data.get('text', '')
    media = data.get('media', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'media_count': len(media), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature}
    }
    
    try:
        import base64
        # Nova Pro model supports both images and videos
        if media_type == 'image':
            content = [{"text": text}]
            for img_base64 in media:
                content.append({
                    "image": {
                        "format": "jpeg",
                        "source": {"bytes": img_base64}
                    }
                })
        elif media_type == 'video':
            # å¤„ç†è§†é¢‘ï¼šæå–å…³é”®å¸§å¹¶è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼
            if len(media) > 0:
                video_base64 = media[0]  # å‡è®¾åªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
                logging.info("Novaå¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
                keyframes = extract_frames_from_video(video_base64, num_frames=8)
                
                if keyframes:
                    # å¢å¼ºæç¤ºè¯ä»¥è¯´æ˜è¿™äº›æ˜¯è§†é¢‘å…³é”®å¸§
                    enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                    content = [{"text": enhanced_text}]
                    
                    # æ·»åŠ å…³é”®å¸§ä½œä¸ºå›¾ç‰‡
                    for frame_base64 in keyframes:
                        content.append({
                            "image": {
                                "format": "jpeg",
                                "source": {"bytes": frame_base64}
                            }
                        })
                    logging.info(f"NovaæˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
                else:
                    return jsonify({"error": "Failed to extract frames from video for Nova processing."}), 400
            else:
                return jsonify({"error": "No video data received for Nova."}), 400
        else:
            return jsonify({"error": "Unsupported media type"}), 400
        
        request_body = {
            "schemaVersion": "messages-v1",
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "inferenceConfig": {
                "max_new_tokens": max_tokens,
                "temperature": temperature
            }
        }
        
        log_entry['bedrock_request'] = {k: v if k != 'messages' else 'omitted' for k, v in request_body.items()}
        
        # åŠ¨æ€æ„å»ºNovaæ¨ç†é…ç½®æ–‡ä»¶ARN
        model_id = build_inference_profile_arn('nova')
        logging.info(f"Using Nova model ID: {model_id}")
        
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        
        response_body = json.loads(response['body'].read())
        log_entry['response'] = response_body
        logging.info(json.dumps(log_entry, ensure_ascii=False))
        return jsonify(response_body)
        
    except Exception as e:
        log_entry['error'] = str(e)
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        return jsonify({"error": str(e)}), 500

def call_emd_model_internal(data, model_key):
    """EMD model inference internal function"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    if model_key not in EMD_MODELS:
        raise Exception(f"Unsupported EMD model: {model_key}")
    
    model_id = EMD_MODELS[model_key]
    
    log_entry = {
        'time': datetime.now().isoformat(),
        'request': {'text': text, 'frames_count': len(frames), 'media_type': media_type, 'max_tokens': max_tokens, 'temperature': temperature, 'model': model_id}
    }
    
    try:
        logging.info(f"ğŸš€ Starting EMD inference for model {model_id}")
        
        # Try OpenAI client first (API endpoint)
        logging.info(f"ğŸ” Checking EMD OpenAI client availability...")
        openai_client = create_emd_openai_client()
        if openai_client:
            logging.info(f"âœ… EMD OpenAI client available, proceeding with inference")
            return call_emd_via_openai(openai_client, data, model_id, log_entry)
        
        # Fallback to SageMaker client
        logging.info(f"ğŸ” Checking EMD SageMaker client availability...")
        sagemaker_client = create_emd_sagemaker_client(model_id)
        if sagemaker_client:
            logging.info(f"âœ… EMD SageMaker client available, proceeding with inference")
            return call_emd_via_sagemaker(sagemaker_client, data, model_id, log_entry)
        
        # No client available - provide helpful error message
        logging.error(f"âŒ No EMD client available for {model_id}")
        error_msg = f"EMD model {model_id} is not deployed yet. Please deploy the model first using: emd deploy --model-id {model_id} --instance-type g5.12xlarge --engine-type vllm --service-type sagemaker_realtime --model-tag dev"
        raise Exception(error_msg)
        
    except Exception as e:
        log_entry['error'] = str(e)
        logging.error(json.dumps(log_entry, ensure_ascii=False))
        raise Exception(str(e))

def call_emd_via_openai(client, data, model_id, log_entry):
    """Call EMD model via OpenAI API"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    logging.info(f"ğŸ¯ Processing {media_type} input for EMD model {model_id}")
    messages = []
    
    if media_type == 'video':
        # Process video: extract keyframes
        if len(frames) > 0:
            video_base64 = frames[0]
            logging.info(f"ğŸ“¹ EMD {model_id} å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
            keyframes = extract_frames_from_video(video_base64, num_frames=8)
            
            if keyframes:
                enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                content = [{"type": "text", "text": enhanced_text}]
                
                for frame_base64 in keyframes:
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame_base64}"
                        }
                    })
                
                messages.append({
                    "role": "user",
                    "content": content
                })
                logging.info(f"âœ… EMD {model_id} æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
            else:
                raise Exception("Failed to extract frames from video for EMD processing")
        else:
            raise Exception("No video data received for EMD")
    else:
        # Process images
        logging.info(f"ğŸ–¼ï¸ Processing {len(frames)} images for EMD model {model_id}")
        content = [{"type": "text", "text": text}]
        for img_base64 in frames:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{img_base64}"
                }
            })
        
        messages.append({
            "role": "user",
            "content": content
        })
    
    logging.info(f"ğŸ”¥ Calling EMD OpenAI API for {model_id}/dev...")
    response = client.chat.completions.create(
        model=f"{model_id}/dev",
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature
    )
    
    logging.info(f"âœ… EMD OpenAI API call completed for {model_id}")
    log_entry['response'] = response.model_dump()
    logging.info(json.dumps(log_entry, ensure_ascii=False))
    
    return {
        "content": [{"text": response.choices[0].message.content}],
        "usage": response.usage.model_dump() if response.usage else None
    }

def call_emd_via_sagemaker(client, data, model_id, log_entry):
    """Call EMD model via SageMaker SDK"""
    text = data.get('text', '')
    frames = data.get('frames', [])
    media_type = data.get('mediaType', 'image')
    max_tokens = data.get('max_tokens', 1024)
    temperature = data.get('temperature', 0.1)
    
    logging.info(f"ğŸ¯ Processing {media_type} input for EMD SageMaker model {model_id}")
    messages = []
    
    if media_type == 'video':
        # Process video: extract keyframes
        if len(frames) > 0:
            video_base64 = frames[0]
            logging.info(f"ğŸ“¹ EMD SageMaker {model_id} å¼€å§‹ä»è§†é¢‘æå–å…³é”®å¸§...")
            keyframes = extract_frames_from_video(video_base64, num_frames=8)
            
            if keyframes:
                enhanced_text = enhance_prompt_for_video(text, len(keyframes))
                content = [{"type": "text", "text": enhanced_text}]
                
                for frame_base64 in keyframes:
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame_base64}"
                        }
                    })
                
                messages.append({
                    "role": "user",
                    "content": content
                })
                logging.info(f"âœ… EMD SageMaker {model_id} æˆåŠŸæ·»åŠ äº† {len(keyframes)} ä¸ªå…³é”®å¸§åˆ°è¯·æ±‚ä¸­")
            else:
                raise Exception("Failed to extract frames from video for EMD SageMaker processing")
        else:
            raise Exception("No video data received for EMD SageMaker")
    else:
        # Process images
        logging.info(f"ğŸ–¼ï¸ Processing {len(frames)} images for EMD SageMaker model {model_id}")
        content = [{"type": "text", "text": text}]
        for img_base64 in frames:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{img_base64}"
                }
            })
        
        messages.append({
            "role": "user",
            "content": content
        })
    
    logging.info(f"ğŸ”¥ Calling EMD SageMaker SDK for {model_id}...")
    response = client.invoke({"messages": messages})
    
    logging.info(f"âœ… EMD SageMaker SDK call completed for {model_id}")
    log_entry['response'] = response
    logging.info(json.dumps(log_entry, ensure_ascii=False))
    
    return response

# EMD model endpoints
@app.route('/api/emd/<model_key>', methods=['POST'])
def call_emd_model(model_key):
    """Generic EMD model endpoint"""
    try:
        result = call_emd_model_internal(request.json, model_key)
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/emd/status', methods=['GET'])
def emd_status():
    """Get EMD deployment status"""
    try:
        result = subprocess.run(['emd', 'status'], capture_output=True, text=True)
        if result.returncode == 0:
            return jsonify({"status": "success", "output": result.stdout})
        else:
            return jsonify({"status": "error", "output": result.stderr}), 500
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/emd/models', methods=['GET'])
def emd_models():
    """Get available EMD models"""
    return jsonify({"models": EMD_MODELS})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000) 