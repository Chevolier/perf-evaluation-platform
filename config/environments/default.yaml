# Default configuration for inference platform

server:
  host: "0.0.0.0"
  port: 5000
  debug: false
  cors_origins: ["*"]

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "data/logs/application.log"
  max_size: 10485760  # 10MB
  backup_count: 5

models:
  emd:
    base_url: ""
    api_key: ""
    timeout: 120
  bedrock:
    region: "us-east-1"
    timeout: 60

benchmarking:
  default_params:
    num_requests: 100
    concurrency: 10
    input_tokens_range: [50, 200]
    output_tokens_range: [100, 500]
    temperature: 0.1
  evalscope:
    environment_path: "/home/ec2-user/SageMaker/efs/conda_envs/evalscope"
    timeout: 600

storage:
  data_dir: "data"
  benchmarks_dir: "data/benchmarks"
  logs_dir: "data/logs"
  temp_dir: "data/temp"
  archive_dir: "/home/ec2-user/SageMaker/efs/Projects/llm-performance-viz/archive_results"