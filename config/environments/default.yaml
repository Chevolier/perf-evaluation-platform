# Default configuration for inference platform

server:
  host: "0.0.0.0"
  port: 5000
  debug: false
  workers: 1

logging:
  level: "INFO"
  file: "logs/app.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5

aws:
  region: "us-east-1"
  account_id: null  # Auto-detected
  profile: null
  access_key_id: null
  secret_access_key: null
  session_token: null

models:
  emd:
    base_url: "http://localhost:8000"
    default_tag: null
    timeout: 300
  bedrock:
    region: "us-west-2"
    timeout: 300

database:
  path: "data/app.db"
  echo: false

storage:
  base_path: "outputs"
  cleanup_days: 30

hyperpod:
  infraforge_root: "../InfraForge"
  deploy_script: "scripts/deploy_hyperpod.sh"
  destroy_script: "scripts/deploy_hyperpod.sh"
  presets:
    small: "configs/hyperpod/config_hyperpod_small.yaml"
    medium: "configs/hyperpod/config_hyperpod_medium.yaml"
    large: "configs/hyperpod/config_hyperpod_large.yaml"
  dry_run: true
  log_directory: "logs/hyperpod"
  default_region: "us-west-2"

benchmarking:
  default_timeout: 1800  # 30 minutes
  max_concurrent: 5
  result_retention_days: 90

cors:
  origins:
    - "http://localhost:3000"
    - "http://localhost:3001"
  methods:
    - "GET"
    - "POST" 
    - "PUT"
    - "DELETE"
    - "OPTIONS"
  allow_headers:
    - "Content-Type"
    - "Authorization"
