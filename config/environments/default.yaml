# Default configuration for inference platform

server:
  host: "0.0.0.0"
  port: 5000
  debug: false
  workers: 1

logging:
  level: "INFO"
  file: "logs/app.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5

aws:
  region: "us-west-2"
  account_id: null  # Auto-detected

models:
  ec2:
    default_port: 8000
    timeout: 300
    health_check_interval: 10
    deployment_timeout: 600
  bedrock:
    region: "us-west-2"
    timeout: 300

database:
  path: "data/app.db"
  echo: false

storage:
  base_path: "outputs"
  cleanup_days: 30

benchmarking:
  default_timeout: 1800  # 30 minutes
  max_concurrent: 5
  result_retention_days: 90

cors:
  origins:
    - "http://localhost:3000"
    - "http://localhost:3001"
  methods:
    - "GET"
    - "POST" 
    - "PUT"
    - "DELETE"
    - "OPTIONS"
  allow_headers:
    - "Content-Type"
    - "Authorization"
